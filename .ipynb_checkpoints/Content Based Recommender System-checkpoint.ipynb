{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sps\n",
    "%matplotlib inline\n",
    "\n",
    "#train_final.csv - the training set of interactions\n",
    "train_final = pd.read_csv('input/train_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#tracks_final.csv - supplementary information about the items\n",
    "tracks_final = pd.read_csv('input/tracks_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#playlists_final.csv - supplementary information about the users\n",
    "playlists_final = pd.read_csv('input/playlists_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#target_playlists.csv - the set of target playlists that will receive recommendations\n",
    "target_playlists = pd.read_csv('input/target_playlists.csv');\n",
    "\n",
    "#target_tracks.csv - the set of target items (tracks) to be recommended\n",
    "target_tracks = pd.read_csv('input/target_tracks.csv');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41756, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we want to remove some redundant stuff. \n",
    "\n",
    "#We will remove all songs which are not occurring more than 10 times in train_final\n",
    "#Nevertheless, we still want to keep all tracks which are in the target tracks.  \n",
    "\n",
    "popularity = train_final.groupby(by=\"track_id\").playlist_id.nunique().to_frame()\n",
    "\n",
    "#remove index name\n",
    "popularity.reset_index(level = 0, inplace = True)\n",
    "\n",
    "#Rename the columns\n",
    "popularity.columns = ['track_id','occurrences']\n",
    "\n",
    "#Remove all targeted tracks - TESTED, working as expected\n",
    "tracks_relevant = popularity[~popularity['track_id'].isin(target_tracks['track_id'])]\n",
    "\n",
    "#Remove tracks occurring less than 10 times\n",
    "tracks_relevant = tracks_relevant[tracks_relevant['occurrences'] > 10]\n",
    "\n",
    "#Add the targeteted tracks back again\n",
    "tracks_relevant = pd.concat([tracks_relevant, target_tracks])\n",
    "\n",
    "tracks_relevant.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create train_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<57561x41756 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a User Rating Matrix. In our case, User is represented by playlist\n",
    "#And the ratings are implicit binary, 1/0 depending on wether a song is in the playlist. \n",
    "\n",
    "URM_all = sps.csr_matrix(np.zeros((playlists_final.shape[0], tracks_relevant.shape[0])))\n",
    "\n",
    "URM_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>playcount</th>\n",
       "      <th>album</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2972914</td>\n",
       "      <td>144</td>\n",
       "      <td>224000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[54087, 1757, 1718, 116712, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750239</td>\n",
       "      <td>246</td>\n",
       "      <td>157000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[189631, 3424, 177424, 46208, 205245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550729</td>\n",
       "      <td>144</td>\n",
       "      <td>217000</td>\n",
       "      <td>554.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 46869, 183258, 54337]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2169950</td>\n",
       "      <td>144</td>\n",
       "      <td>207000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 70618, 207003, 109806, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1903709</td>\n",
       "      <td>144</td>\n",
       "      <td>198000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[54087, 81223, 116712, 215342, 71028]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id  artist_id  duration  playcount   album  \\\n",
       "0   2972914        144    224000       49.0     [7]   \n",
       "1   2750239        246    157000        1.0     [8]   \n",
       "2   1550729        144    217000      554.0     [9]   \n",
       "3   2169950        144    207000      200.0     [9]   \n",
       "4   1903709        144    198000        5.0  [None]   \n",
       "\n",
       "                                     tags  \n",
       "0     [54087, 1757, 1718, 116712, 189631]  \n",
       "1   [189631, 3424, 177424, 46208, 205245]  \n",
       "2   [54087, 109806, 46869, 183258, 54337]  \n",
       "3  [54087, 70618, 207003, 109806, 116712]  \n",
       "4   [54087, 81223, 116712, 215342, 71028]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets take a look at the tags.\n",
    "tracks_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31900\n"
     ]
    }
   ],
   "source": [
    "#Lets translate the tags to indexes. \n",
    "tracks_final['tags'].head()\n",
    "\n",
    "\n",
    "tags_indexes = {}\n",
    "counter = 0; #We will start at 1, reserving col 0 for indexes. \n",
    "for row in tracks_final['tags']:\n",
    "    tags = row.strip('[ ]').split(', ')\n",
    "    for tag in tags:\n",
    "        if len(tag) > 0: \n",
    "            tag = int(tag)\n",
    "            if not(tag in tags_indexes):\n",
    "                tags_indexes[tag] = counter #Lets make it int to make it easier\n",
    "                counter += 1;\n",
    "\n",
    "print(len(tags_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100000 unique tracks with 31900 unique tags\n"
     ]
    }
   ],
   "source": [
    "#If we translate each track_id to a track_index which will serve as matrix index, we can save a lot of time. \n",
    "\n",
    "\n",
    "#We need a way to get from track_id to index in O(1).\n",
    "#Let's create a dictionary\n",
    "\n",
    "tracks_indexes = {}\n",
    "index_to_track_id = {}\n",
    "counter = 0; #We will start at 1, reserving col 0 for indexes.  \n",
    "for track_id in tracks_final['track_id']:\n",
    "    tracks_indexes[track_id] = counter\n",
    "    index_to_track_id[counter] = int(track_id)\n",
    "    counter += 1;\n",
    "    \n",
    "    \n",
    "print(\"We have {} unique tracks with {} unique tags\".format(len(tracks_indexes), len(tags_indexes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#and a way to get from playlist_id to index in O(1)\n",
    "\n",
    "\n",
    "playlist_indexes = {}\n",
    "index_to_playlist = {}\n",
    "counter = 0; \n",
    "for playlist_id in playlists_final['playlist_id']:\n",
    "    playlist_indexes[playlist_id] = counter\n",
    "    counter += 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 31900)\n"
     ]
    }
   ],
   "source": [
    "#Now we can create an Item Content Matrix. \n",
    "\n",
    "ICM_all = np.zeros((len(tracks_indexes), len(tags_indexes)), int)\n",
    "print(ICM_all.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   occurrences  track_id  artist_id  duration  playcount     album  \\\n",
      "0         18.0       360     169649    194000      522.0   [77662]   \n",
      "1         11.0      1376     381303    270000      629.0  [180587]   \n",
      "2         12.0      2623      36019    329000     7081.0   [17295]   \n",
      "3         15.0      2891     310373    194000      196.0  [142650]   \n",
      "4         14.0      2901     163720    293000      322.0   [74384]   \n",
      "\n",
      "                                      tags  \n",
      "0      [70618, 70251, 70625, 25307, 11056]  \n",
      "1  [205245, 254105, 11056, 209598, 189631]  \n",
      "2    [122769, 23214, 48976, 117167, 90254]  \n",
      "3     [189631, 54087, 70618, 94794, 61837]  \n",
      "4     [193464, 205245, 46208, 92324, 3982]  \n"
     ]
    }
   ],
   "source": [
    "#tracks_relevant.reset_index(level = 0, inplace = True)\n",
    "\n",
    "tracks = pd.merge(tracks_relevant, tracks_final, how='inner', on='track_id')\n",
    "\n",
    "print(tracks.head())\n",
    "\n",
    "tracks = tracks.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#So let's fill it with our data.\n",
    "\n",
    "\n",
    "for row in tracks: \n",
    "    track_id = row[1]\n",
    "    tags = row[6].strip('[ ]').split(', ')\n",
    "    for tag in tags:\n",
    "        if len(tag) > 0:\n",
    "            tag_index = tags_indexes[int(tag)]\n",
    "            track_index = tracks_indexes[track_id]\n",
    "            ICM_all[track_index,tag_index] = 1\n",
    "            #print(\"Added tag {} to track {}\".format(tag, track_id))\n",
    "            \n",
    "\n",
    "print(ICM_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x31900 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 203506 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICM_all_sparse = sps.csr_matrix(ICM_all)\n",
    "ICM_all_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets take a look at the data. \n",
    "\n",
    "features_per_item = (ICM_all_sparse > 0).sum(axis=1)\n",
    "items_per_feature = (ICM_all_sparse > 0).sum(axis=0)\n",
    "\n",
    "features_per_item = np.sort(features_per_item)\n",
    "items_per_feature = np.sort(items_per_feature)\n",
    "\n",
    "print(features_per_item.shape)\n",
    "print(items_per_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040522, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets build that URM matrix. Should be nofplaylists x nofitems\n",
    "\n",
    "item_playlist_matrix = np.zeros([playlists_final.shape[0], tracks_final.shape[0]],int) \n",
    "\n",
    "interactions = train_final.as_matrix()\n",
    "for row in interactions:\n",
    "    #Lets get the info\n",
    "    playlist_id = row[0]\n",
    "    track_id = row[1]\n",
    "    \n",
    "    #Now lets get the proper indexes. \n",
    "    playlist_index = playlist_indexes[playlist_id]\n",
    "    track_index = tracks_indexes[track_id]\n",
    "    \n",
    "    #And now lets add it to the matrix\n",
    "    item_playlist_matrix[playlist_index][track_index] = 1\n",
    "    \n",
    "\n",
    "print(item_playlist_matrix.shape)\n",
    "print(\"hej1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57560, 100000)\n",
      "(1040522,)\n",
      "(1040522,)\n"
     ]
    }
   ],
   "source": [
    "#Let's Split the Data and create evluation functions. \n",
    "\n",
    "train_test_split = 0.80\n",
    "\n",
    "numInteractions = train_final.shape[0]\n",
    "\n",
    "\n",
    "train_mask = np.random.choice([True,False], numInteractions, [train_test_split, 1-train_test_split])\n",
    "\n",
    "playlistList = np.zeros((train_final['playlist_id'].shape[0], 1), int)\n",
    "playlistList[:, 0] = train_final['playlist_id']\n",
    "\n",
    "#itemList = np.zeros((train_final['track_id'].shape[0], 1), int)\n",
    "#itemList[:, 0] = train_final['track_id']\n",
    "playlistList = train_final['playlist_id'].values\n",
    "itemList = train_final['track_id'].values\n",
    "\n",
    "#Translate ids\n",
    "playlistList_translated = np.zeros(playlistList.shape)\n",
    "itemList_translated = np.zeros(itemList.shape)\n",
    "for i in range(train_final.shape[0]):\n",
    "    playlistList_translated[i] = playlist_indexes[playlistList[i]]\n",
    "    itemList_translated[i] = tracks_indexes[itemList[i]]\n",
    "\n",
    "\n",
    "\n",
    "ratingList = np.ones((playlistList.shape), int)\n",
    "\n",
    "\n",
    "URM_train = sps.coo_matrix((ratingList[train_mask], (playlistList_translated[train_mask], itemList_translated[train_mask])))\n",
    "URM_train = URM_train.tocsr()\n",
    "\n",
    "print(URM_train.shape)\n",
    "print(playlistList.shape)\n",
    "print(itemList.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_mask = np.logical_not(train_mask)\n",
    "\n",
    "URM_test = sps.coo_matrix((ratingList[test_mask], (playlistList[test_mask], itemList[test_mask])))\n",
    "URM_test = URM_test.tocsr()\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, at=5):\n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    for i,user_id in  enumerate(userList_unique):\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(\"User %d of %d\" % (i, len(userList_unique)))\n",
    "\n",
    "        relevant_items = URM_test[user_id].indices\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommender_object.recommend(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "class BasicItemKNNRecommender(object):\n",
    "    \"\"\" ItemKNN recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM, k=50, shrinkage=100, similarity='cosine'):\n",
    "        self.dataset = URM\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'pearson':\n",
    "            self.distance = Pearson(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'adj-cosine':\n",
    "            self.distance = AdjustedCosine(shrinkage=self.shrinkage)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ItemKNN(similarity={},k={},shrinkage={})\".format(\n",
    "            self.similarity_name, self.k, self.shrinkage)\n",
    "\n",
    "    def fit(self, X):\n",
    "        item_weights = self.distance.compute(X)\n",
    "        \n",
    "        item_weights = check_matrix(item_weights, 'csr') # nearly 10 times faster\n",
    "        print(\"Converted to csr\")\n",
    "        \n",
    "        # for each column, keep only the top-k scored items\n",
    "        # THIS IS THE SLOW PART, FIND A BETTER SOLUTION        \n",
    "        values, rows, cols = [], [], []\n",
    "        nitems = self.dataset.shape[1]\n",
    "        for i in range(nitems):\n",
    "            if (i % 10000 == 0):\n",
    "                print(\"Item %d of %d\" % (i, nitems))\n",
    "                \n",
    "            this_item_weights = item_weights[i,:].toarray()[0]\n",
    "            top_k_idx = np.argsort(this_item_weights) [-self.k:]\n",
    "                        \n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(nitems)[top_k_idx])\n",
    "            cols.extend(np.ones(self.k) * i)\n",
    "        self.W_sparse = sps.csc_matrix((values, (rows, cols)), shape=(nitems, nitems), dtype=np.float32)\n",
    "\n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.dataset[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(user_id, ranking)\n",
    "            \n",
    "        export = [0,0,0,0,0]\n",
    "        for i in range(5):\n",
    "            t_id = index_to_track_id[ranking[i]]\n",
    "            export[i] = t_id\n",
    "            \n",
    "        return export\n",
    "    def _filter_seen(self, user_id, ranking):\n",
    "        user_profile = self.dataset[user_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "class ISimilarity(object):\n",
    "    \"\"\"Abstract interface for the similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, shrinkage=10):\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def compute(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cosine(ISimilarity):\n",
    "    def compute(self, X):\n",
    "        # convert to csc matrix for faster column-wise operations\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "\n",
    "        # 1) normalize the columns in X\n",
    "        # compute the column-wise norm\n",
    "        # NOTE: this is slightly inefficient. We must copy X to compute the column norms.\n",
    "        # A faster solution is to  normalize the matrix inplace with a Cython function.\n",
    "        Xsq = X.copy()\n",
    "        Xsq.data **= 2\n",
    "        norm = np.sqrt(Xsq.sum(axis=0))\n",
    "        norm = np.asarray(norm).ravel()\n",
    "        norm += 1e-6\n",
    "        # compute the number of non-zeros in each column\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(X.indptr)\n",
    "        # then normalize the values in each column\n",
    "        X.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "\n",
    "        # 2) compute the cosine similarity using the dot-product\n",
    "        dist = X * X.T\n",
    "        print(\"Computed\")\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        dist = dist - sps.dia_matrix((dist.diagonal()[scipy.newaxis, :], [0]), shape=dist.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            dist = self.apply_shrinkage(X, dist)\n",
    "            print(\"Applied shrinkage\")    \n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57560, 100000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Converted to csr\n",
      "Item 0 of 100000\n",
      "Item 10000 of 100000\n",
      "Item 20000 of 100000\n",
      "Item 30000 of 100000\n",
      "Item 40000 of 100000\n",
      "Item 50000 of 100000\n",
      "Item 60000 of 100000\n",
      "Item 70000 of 100000\n",
      "Item 80000 of 100000\n",
      "Item 90000 of 100000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "rec = BasicItemKNNRecommender(URM=URM_train, shrinkage=0.0, k=50)\n",
    "rec.fit(ICM_all_sparse)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ICM_all = ICM_all_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 31900)\n",
      "(31900,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "num_tot_items = ICM_all.shape[0]\n",
    "\n",
    "# let's count how many items have a certain feature\n",
    "items_per_feature = (ICM_all > 0).sum(axis=0)\n",
    "\n",
    "IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "\n",
    "print(ICM_all.shape)\n",
    "print(IDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31900,)\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "ICM_idf = sps.csr_matrix(ICM_all, dtype=np.float64)\n",
    "# compute the number of non-zeros in each col\n",
    "# NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "col_nnz = np.diff(check_matrix(ICM_idf, 'csc').indptr)\n",
    "print(col_nnz.shape)\n",
    "print(ICM_idf.shape)\n",
    "print(IDF.shape)\n",
    "# then normalize the values in each col\n",
    "ICM_idf.data *= np.repeat(IDF, col_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Converted to csr\n",
      "Item 0 of 100000\n",
      "Item 10000 of 100000\n",
      "Item 20000 of 100000\n",
      "Item 30000 of 100000\n",
      "Item 40000 of 100000\n",
      "Item 50000 of 100000\n",
      "Item 60000 of 100000\n",
      "Item 70000 of 100000\n",
      "Item 80000 of 100000\n",
      "Item 90000 of 100000\n"
     ]
    }
   ],
   "source": [
    "rec_idf = BasicItemKNNRecommender(URM=URM_train, shrinkage=0.0, k=50)\n",
    "rec_idf.fit(ICM_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10000 playlisys\n",
      "1000 out of 10000 playlisys\n",
      "2000 out of 10000 playlisys\n",
      "3000 out of 10000 playlisys\n",
      "4000 out of 10000 playlisys\n",
      "5000 out of 10000 playlisys\n",
      "6000 out of 10000 playlisys\n",
      "7000 out of 10000 playlisys\n",
      "8000 out of 10000 playlisys\n",
      "9000 out of 10000 playlisys\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "zeros = np.zeros((target_playlists.size, 6), dtype = int)\n",
    "recommendations = pd.DataFrame(zeros)\n",
    "recommendations.columns = ['playlist_id', 1, 2, 3, 4, 5]\n",
    "\n",
    "counter = 0\n",
    "for playlist_id in target_playlists['playlist_id']:\n",
    "    \n",
    "    if counter % 1000 == 0: \n",
    "        print (\"%s out of 10000 playlists\" %(counter))\n",
    "    \n",
    "    \n",
    "    playlist_id_translated = playlist_indexes[int(playlist_id)]\n",
    "    #print(rec.recommend(playlist_id_translated, at=5))\n",
    "    recommendations.iloc[counter, 1:6] = rec.recommend(playlist_id_translated, at=5)\n",
    "    recommendations.iloc[counter, 0] = playlist_id\n",
    "    counter += 1\n",
    "    \n",
    "#print(recommendations)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       playlist_id        1        2        3        4        5\n",
      "0        10024884  3156505  1469927  3063256   848214    60264\n",
      "1        10624787   194963  2319873  4924281   497324  1818963\n",
      "2         4891851   557629  4735764  2526786  2940539  1809338\n",
      "3         4267369  1154985  3137640   139177  1942809  3658983\n",
      "4           65078   509377   119305  2863566  1742595  2386315\n",
      "5        10637124   346386  2340644  2123901  2740526  1633063\n",
      "6         3223162   153334  1820710   498087  1438727  2874676\n",
      "7         7541503  1295068    76828  3738541  2456972  2086779\n",
      "8         6189367  2019507  1054050  1195554  3051311  3869194\n",
      "9         8459943  3536664  1003537  2485522  3129861   826298\n",
      "10       10138804  1388061  2744692  1229845   468510  2515331\n",
      "11       10562075  3708968   683230  1212246  3593525  1881344\n",
      "12       10184821  3194368  1253640  2426923  2513863   819539\n",
      "13        4189678  1175499   511452  2944080   253512  1796061\n",
      "14        6299524  1744414  3464297   887493  2932680  3056586\n",
      "15        8515028  1464766   247759  2811494  3884381   579602\n",
      "16       10631638  1955332   229042  1103473    84222  1828654\n",
      "17       10947423  1977730   134529  3332221  2730469  2371612\n",
      "18       11295404   830634  1163927  2991078   166429  3150033\n",
      "19        3884714  3889627  2516311   130965   641566  1251918\n",
      "20       10680851  1563133  3044529  3098639  1249391   509003\n",
      "21        3228268   193666  3870306  1678109  1301377  2976640\n",
      "22        7850055  3502189  3716483  1479795   950776  3697197\n",
      "23       10864017  3888870  2710093  3076244  2220019  3842993\n",
      "24        5660760  2277502  2856374   727914   632293   495072\n",
      "25        4157419   722020  2719191  2628118  1293892  2178283\n",
      "26        3304833   641607  2693619   673880  1130644   648467\n",
      "27        1911645  2110926  3470420  3554610  3569315  1736009\n",
      "28        4902536  2467218  3581227  2160491  1216683  2028674\n",
      "29        1140304    12933  1620771  2902983  1993929   528692\n",
      "...           ...      ...      ...      ...      ...      ...\n",
      "9970     11381473  2586600  3435765  2563989   206564   520226\n",
      "9971      3520121  2965538  3287197  1616057  1848106   512230\n",
      "9972      6612439  2569103  1863293  2833627  2923846  2438796\n",
      "9973      6218231  3526742   707102  3159698   205324  2349056\n",
      "9974      8344678  3534697  1184839  1787174  1855407  1166047\n",
      "9975      7470841  3583305   950760   583254  1457317  3792998\n",
      "9976      6423094  2695825   932805  3306432   323372  2151227\n",
      "9977      8139437   846431   189736  2263419  3191103   513417\n",
      "9978      8227203   180331  2268115   190241   717073  3581204\n",
      "9979      5903507  1778792  3113425   394245  2750825  3735223\n",
      "9980      3778920   456255  3042317  2320105  1134602  2926620\n",
      "9981      1061175    20064  3672178    48329   864246   876465\n",
      "9982      8572406  1519355   180276  2490193  1689006  1566316\n",
      "9983     10413491  3682536  1366440  3114571  1197144  3309112\n",
      "9984      8868869   539410   293152  2357910  3425632  3550611\n",
      "9985      8202309   153171   166974    91684  3231335  2050425\n",
      "9986      7784396  1105355  2305744  1102487  3068491  1534952\n",
      "9987      4622927  1349043  2820187  2427465   565465  2396816\n",
      "9988     11542825  2739985  3175815   674874  1469981  1598426\n",
      "9989      7138440  3254754  3753911  3633793  3836979   850399\n",
      "9990      5146771   203645  2119016   467459  3689726  2190061\n",
      "9991      4506918  2493856  3563894  3856976  1277629   316015\n",
      "9992     10337087  2229130  1457047  2738980   692988  2883861\n",
      "9993      5608217  2050425  3231335  1672495    91684  1051939\n",
      "9994      7697108  1966489    43827  4497502   417901  3674307\n",
      "9995     11616286   498709   230528  4895132  3227737  1914261\n",
      "9996        53201  3076030  3184131  3337526   962932  1044692\n",
      "9997     11369546  2786860  3160996  2134657  1832371  1936235\n",
      "9998      7939535  2750182   689983  1188711   268245   715133\n",
      "9999       297021   675385  3242991  1811008   234619  2209246\n",
      "\n",
      "[10000 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "def save_to_file():\n",
    "    #Saves the recommendations dataframe to the .csv-file. \n",
    "    np.savetxt(\"recommendations_idf.csv\",recommendations, fmt = '%s,%s %s %s %s %s', header = \"playlist_id,track_ids\", newline = \"\\n\")\n",
    "    \n",
    "    \n",
    "def test():\n",
    "    #Do something\n",
    "    print(\"Result: \")\n",
    "    pass\n",
    "\n",
    "\n",
    "save_to_file()\n",
    "print(recommendations.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
