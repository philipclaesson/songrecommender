{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sps\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "#train_final.csv - the training set of interactions\n",
    "train_final = pd.read_csv('input/train_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#tracks_final.csv - supplementary information about the items\n",
    "tracks_final = pd.read_csv('input/tracks_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#playlists_final.csv - supplementary information about the users\n",
    "playlists_final = pd.read_csv('input/playlists_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#target_playlists.csv - the set of target playlists that will receive recommendations\n",
    "target_playlists = pd.read_csv('input/target_playlists.csv');\n",
    "\n",
    "#target_tracks.csv - the set of target items (tracks) to be recommended\n",
    "target_tracks = pd.read_csv('input/target_tracks.csv');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Algorithm still not as good as it should be. \n",
    "#\n",
    "#Bug hunting: \n",
    "#    - Bad content tags? Yes, alNone was a tag. Removed but still not improved. \n",
    "#    - Bad URM. The URM was only trained with 50% of data. Now fixed, improved a lot. \n",
    "#    - \n",
    "#    - \n",
    "#    - \n",
    "#    - \n",
    "#    - \n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_final now contains 945579 interactions. \n",
      "Tracks_final now contains 74542 tracks. \n"
     ]
    }
   ],
   "source": [
    "#This step is not needed yet, will make ratings worse! \n",
    "\n",
    "def get_relevant_tracks():\n",
    "    #Now we want to remove some redundant stuff. \n",
    "\n",
    "    #We will remove all songs which are not occurring more than 10 times in train_final\n",
    "    #Nevertheless, we still want to keep all tracks which are in the target tracks.  \n",
    "\n",
    "    popularity = train_final.groupby(by=\"track_id\").playlist_id.nunique().to_frame()\n",
    "\n",
    "    #remove index name\n",
    "    popularity.reset_index(level = 0, inplace = True)\n",
    "\n",
    "    #Rename the columns\n",
    "    popularity.columns = ['track_id','occurrences']\n",
    "\n",
    "    #Remove all targeted tracks - TESTED, working as expected\n",
    "    tracks_relevant = popularity[~popularity['track_id'].isin(target_tracks['track_id'])]\n",
    "\n",
    "    #Remove tracks occurring less than 10 times\n",
    "    tracks_relevant = tracks_relevant[tracks_relevant['occurrences'] > 4]\n",
    "\n",
    "    #Add the targeteted tracks back again\n",
    "    tracks_relevant = pd.concat([tracks_relevant, target_tracks])\n",
    "\n",
    "    return(tracks_relevant)\n",
    "\n",
    "    print(\"Removed %s redundant tracks which occured less than 10 times.\" %(tracks_final-tracks_relevant))\n",
    "\n",
    "tracks_relevant = get_relevant_tracks()\n",
    "\n",
    "#Remove irrelevant tracks from train_final and tracks_final\n",
    "train_final = train_final[train_final['track_id'].isin(tracks_relevant['track_id'])]\n",
    "\n",
    "print(\"Train_final now contains %s interactions. \" %(train_final.shape[0]))\n",
    "\n",
    "tracks_final = tracks_final[tracks_final['track_id'].isin(tracks_relevant['track_id'])]\n",
    "\n",
    "print(\"Tracks_final now contains %s tracks. \"%(tracks_final.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>playcount</th>\n",
       "      <th>album</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2972914</td>\n",
       "      <td>144</td>\n",
       "      <td>224000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[54087, 1757, 1718, 116712, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750239</td>\n",
       "      <td>246</td>\n",
       "      <td>157000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[189631, 3424, 177424, 46208, 205245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550729</td>\n",
       "      <td>144</td>\n",
       "      <td>217000</td>\n",
       "      <td>554.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 46869, 183258, 54337]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2169950</td>\n",
       "      <td>144</td>\n",
       "      <td>207000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 70618, 207003, 109806, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1903709</td>\n",
       "      <td>144</td>\n",
       "      <td>198000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[54087, 81223, 116712, 215342, 71028]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id  artist_id  duration  playcount   album  \\\n",
       "0   2972914        144    224000       49.0     [7]   \n",
       "1   2750239        246    157000        1.0     [8]   \n",
       "2   1550729        144    217000      554.0     [9]   \n",
       "3   2169950        144    207000      200.0     [9]   \n",
       "4   1903709        144    198000        5.0  [None]   \n",
       "\n",
       "                                     tags  \n",
       "0     [54087, 1757, 1718, 116712, 189631]  \n",
       "1   [189631, 3424, 177424, 46208, 205245]  \n",
       "2   [54087, 109806, 46869, 183258, 54337]  \n",
       "3  [54087, 70618, 207003, 109806, 116712]  \n",
       "4   [54087, 81223, 116712, 215342, 71028]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets take a look at the tags.\n",
    "tracks_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Translating all content ids into indexes.\n",
    "\n",
    "#We need to create buckets for the playcount and duration. \n",
    "#Lets create buckets and a help function for the duration. \n",
    "\n",
    "n_duration_buckets = 3\n",
    "def duration_to_bucket(duration, alternative = 2):\n",
    "    if (alternative == 1):\n",
    "        n_duration_buckets = 8\n",
    "        if duration <= 0:\n",
    "            print(\"Null duration reached bucket function. \")\n",
    "            return None\n",
    "        elif duration < 90000: #not a song\n",
    "            return 1\n",
    "        elif duration < 140000: #short song\n",
    "            return 2\n",
    "        elif duration < 220000: #radio song\n",
    "            return 3\n",
    "        elif duration < 340000: #normal song\n",
    "            return 4\n",
    "        elif duration < 480000: #long song\n",
    "            return 5\n",
    "        elif duration < 720000: #really long\n",
    "            return 6\n",
    "        elif duration < 1200000: #super long\n",
    "            return 7\n",
    "        elif duration >= 1200000: #mixtape/compilation\n",
    "            return 8\n",
    "    elif(alternative == 2):\n",
    "        n_duration_buckets = 3\n",
    "        if duration <= 0:\n",
    "            print(\"Null duration reached bucket function. \")\n",
    "            return None\n",
    "        elif duration <= 150000: #very short\n",
    "            return 1\n",
    "        elif duration > 150000 and duration < 720000: #very long\n",
    "            return 2\n",
    "        elif duration >= 720000: #mixtape/compilation\n",
    "            return 3\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "\n",
    "n_playcount_buckets = 7\n",
    "def playcount_to_bucket(playcount):\n",
    "    if playcount <= 0 or playcount is None:\n",
    "        print(\"Null playcount reached bucket function. \")\n",
    "        return None\n",
    "    elif playcount < 254: #0,4 percentile not popular\n",
    "        return 1\n",
    "    elif playcount < 881: #0,6 perc: known\n",
    "        return 2\n",
    "    elif playcount < 1560: #0,7 popular\n",
    "        return 3\n",
    "    elif playcount < 2808: #0,8 very popular\n",
    "        return 4\n",
    "    elif playcount < 5900: #0,9 hits\n",
    "        return 5\n",
    "    elif playcount < 10494: #0,95 super hits\n",
    "        return 6\n",
    "    elif playcount >= 10494: # mega hits\n",
    "        return 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77040\n",
      "27604 albums. 27607 expected.\n",
      "17536 artists. 17537 expected.\n"
     ]
    }
   ],
   "source": [
    "tracks_final['tags'].head()\n",
    "\n",
    "content_to_index = {}\n",
    "content_to_id = {}\n",
    "content_counter = 0\n",
    "\n",
    "#Lets translate the tags to indexes.\n",
    "for row in tracks_final['tags']:\n",
    "    tags = row.strip('[ ]').split(', ')\n",
    "    for tag in tags:\n",
    "        if len(tag) > 0: \n",
    "            tag = \"t\"+tag\n",
    "            if not(tag in content_to_index):\n",
    "                content_to_index[tag] = content_counter\n",
    "                content_to_id[content_counter] = tag\n",
    "                content_counter += 1;\n",
    "                \n",
    "#Lets translate album into indexes\n",
    "albumcount = 0 # 27607\n",
    "for album in tracks_final['album']:\n",
    "    album = album.strip('[ ]')\n",
    "    if album != None and album != \"None\" and len(album) > 0: #None should not be considered content\n",
    "        album = \"al\"+album\n",
    "        if album == \"alNone\":\n",
    "            print(album)\n",
    "        if not(album in content_to_index):\n",
    "            content_to_index[album] = content_counter\n",
    "            content_to_id[content_counter] = album\n",
    "            content_counter += 1\n",
    "            albumcount += 1\n",
    "\n",
    "#Lets translate artist_id into indexes \n",
    "artistcount = 0 #17537\n",
    "for artist in tracks_final['artist_id']:\n",
    "    artist = str(artist)\n",
    "    if artist != None and artist != \"None\" and len(artist) > 0: #None should not be considered content\n",
    "        artist = \"ar\"+artist\n",
    "        if not(artist in content_to_index):\n",
    "            content_to_index[artist] = content_counter\n",
    "            content_to_id[content_counter] = artist\n",
    "            content_counter += 1\n",
    "            artistcount += 1\n",
    "        \n",
    "\"\"\"\n",
    "#Lets translate the duration buckets into indexes. \n",
    "for bucket in range(n_duration_buckets): \n",
    "    bucket = \"d\"+str(bucket+1)\n",
    "    content_to_index[bucket] = content_counter\n",
    "    content_to_id[content_counter] = bucket\n",
    "    print(\"added %s\" %(bucket))\n",
    "    content_counter += 1\n",
    "\n",
    "#Lets translate the playcount buckets into indexes. \n",
    "for playcount in range(n_playcount_buckets): \n",
    "    playcount = \"p\"+str(playcount+1)\n",
    "    content_to_index[playcount] = content_counter\n",
    "    content_to_id[content_counter] = playcount\n",
    "    \n",
    "    content_counter += 1\n",
    "\n",
    "\n",
    "## Alternative 2: Just one content type per continous variable. \n",
    "#Fun thing to try: can I add all duration/playcounts in one col, normalizing from 0-1? \n",
    "\n",
    "\n",
    "content_to_index[\"duration\"] = content_counter\n",
    "content_to_id[content_counter] = \"duration\"\n",
    "content_counter += 1\n",
    "\n",
    "content_to_index[\"playcount\"] = content_counter\n",
    "content_to_id[content_counter] = \"playcount\"\n",
    "content_counter += 1\n",
    "\"\"\"\n",
    "\n",
    "print(len(content_to_index))\n",
    "print(\"%s albums. 27607 expected.\" %albumcount)\n",
    "print(\"%s artists. 17537 expected.\" %artistcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 57561 playlists with 100000 unique tracks with 77040 unique content types. \n"
     ]
    }
   ],
   "source": [
    "#If we translate each track_id to a track_index which will serve as matrix index, we can save a lot of time. \n",
    "\n",
    "\n",
    "#We need a way to get from track_id to index in O(1).\n",
    "#Let's create a dictionary\n",
    "\n",
    "track_to_id = {}\n",
    "track_to_index = {}\n",
    "track_ids = tracks_final['track_id']\n",
    "\n",
    "counter = 0;\n",
    "for track_id in tracks_final['track_id']:\n",
    "    track_id = int(track_id)\n",
    "    track_to_index[track_id] = counter\n",
    "    track_to_id[counter] = track_id\n",
    "    counter += 1;\n",
    "    \n",
    "#and a way to get from playlist_id to index in O(1)\n",
    "\n",
    "\n",
    "playlist_to_index = {}\n",
    "playlist_to_id = {}\n",
    "counter = 0; \n",
    "for playlist_id in playlists_final['playlist_id']:\n",
    "    playlist_id = int(playlist_id)\n",
    "    playlist_to_index[playlist_id] = counter\n",
    "    playlist_to_id[counter] = playlist_id\n",
    "    counter += 1;\n",
    "    \n",
    "print(\"We have {} playlists with {} unique tracks with {} unique content types. \".format(len(playlist_to_index), len(track_to_index), len(content_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can create an Item Content Matrix. \n",
    "\n",
    "#ICM_all = np.zeros((len(tracks_indexes), len(tags_indexes)), int)\n",
    "#ICM_all = sps.coo_matrix((len(track_to_index), len(content_to_index)), int)\n",
    "#print(ICM_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>playcount</th>\n",
       "      <th>album</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2972914</td>\n",
       "      <td>144</td>\n",
       "      <td>224000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[54087, 1757, 1718, 116712, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750239</td>\n",
       "      <td>246</td>\n",
       "      <td>157000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[189631, 3424, 177424, 46208, 205245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550729</td>\n",
       "      <td>144</td>\n",
       "      <td>217000</td>\n",
       "      <td>554.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 46869, 183258, 54337]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2169950</td>\n",
       "      <td>144</td>\n",
       "      <td>207000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 70618, 207003, 109806, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1903709</td>\n",
       "      <td>144</td>\n",
       "      <td>198000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[54087, 81223, 116712, 215342, 71028]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2256817</td>\n",
       "      <td>144</td>\n",
       "      <td>218000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 189631, 49166, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2561768</td>\n",
       "      <td>928</td>\n",
       "      <td>223000</td>\n",
       "      <td>249.0</td>\n",
       "      <td>[26]</td>\n",
       "      <td>[50764, 4425, 11056, 205245, 81223]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>474864</td>\n",
       "      <td>928</td>\n",
       "      <td>193000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[205245, 81223, 11056, 267, 3982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1378455</td>\n",
       "      <td>928</td>\n",
       "      <td>304000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[11056, 205245, 81223, 189631, 84597]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1523190</td>\n",
       "      <td>928</td>\n",
       "      <td>206000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[205245, 11056, 81223, 4425, 189631]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id  artist_id  duration  playcount   album  \\\n",
       "0   2972914        144    224000       49.0     [7]   \n",
       "1   2750239        246    157000        1.0     [8]   \n",
       "2   1550729        144    217000      554.0     [9]   \n",
       "3   2169950        144    207000      200.0     [9]   \n",
       "4   1903709        144    198000        5.0  [None]   \n",
       "5   2256817        144    218000        2.0     [9]   \n",
       "6   2561768        928    223000      249.0    [26]   \n",
       "7    474864        928    193000       73.0    [22]   \n",
       "8   1378455        928    304000       73.0    [22]   \n",
       "9   1523190        928    206000       10.0    [22]   \n",
       "\n",
       "                                     tags  \n",
       "0     [54087, 1757, 1718, 116712, 189631]  \n",
       "1   [189631, 3424, 177424, 46208, 205245]  \n",
       "2   [54087, 109806, 46869, 183258, 54337]  \n",
       "3  [54087, 70618, 207003, 109806, 116712]  \n",
       "4   [54087, 81223, 116712, 215342, 71028]  \n",
       "5  [54087, 109806, 189631, 49166, 116712]  \n",
       "6     [50764, 4425, 11056, 205245, 81223]  \n",
       "7       [205245, 81223, 11056, 267, 3982]  \n",
       "8   [11056, 205245, 81223, 189631, 84597]  \n",
       "9    [205245, 11056, 81223, 4425, 189631]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 0 of 100000. 0.0 s sec.\n",
      "Track 5000 of 100000. 0.1 s sec.\n",
      "Track 10000 of 100000. 0.21 s sec.\n",
      "Track 15000 of 100000. 0.32 s sec.\n",
      "Track 20000 of 100000. 0.42 s sec.\n",
      "Track 25000 of 100000. 0.54 s sec.\n",
      "Track 30000 of 100000. 0.64 s sec.\n",
      "Track 35000 of 100000. 0.75 s sec.\n",
      "Track 40000 of 100000. 0.86 s sec.\n",
      "Track 45000 of 100000. 0.96 s sec.\n",
      "Track 50000 of 100000. 1.06 s sec.\n",
      "Track 55000 of 100000. 1.16 s sec.\n",
      "Track 60000 of 100000. 1.27 s sec.\n",
      "Track 65000 of 100000. 1.37 s sec.\n",
      "Track 70000 of 100000. 1.48 s sec.\n",
      "Track 75000 of 100000. 1.58 s sec.\n",
      "Track 80000 of 100000. 1.68 s sec.\n",
      "Track 85000 of 100000. 1.8 s sec.\n",
      "Track 90000 of 100000. 1.91 s sec.\n",
      "Track 95000 of 100000. 2.01 s sec.\n",
      "Built ICM matrix with 656745 content values.\n",
      "27604 albums. 27607 expected.\n",
      "17536 artists. 17537 expected.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#So let's fill the ICM with our data.\n",
    "import math\n",
    "\n",
    "def build_ICM():\n",
    "    \n",
    "    no_interactions = train_final.shape[0]\n",
    "    \n",
    "    tracks_matrix = tracks_final.as_matrix()\n",
    "    rows = np.zeros((no_interactions,), dtype = int)\n",
    "    cols = np.zeros((no_interactions,), dtype = int)\n",
    "    val = np.zeros((no_interactions,), dtype = int)\n",
    "    #val[i] = value of row[i] col[i]\n",
    "    #val = []\n",
    "    counter = 0\n",
    "    starttime = time.time()\n",
    "    lasttime = starttime\n",
    "    trackno = 0\n",
    "    addedalbums = {} #for testing\n",
    "    addedartists = {} # for testing\n",
    "    for track in tracks_matrix: \n",
    "        track_id, artist_id, duration, playcount, album, tags = np.split(track, 6)\n",
    "\n",
    "        #Get track index\n",
    "        track_index = track_to_index[int(track_id[0])]\n",
    "\n",
    "        \n",
    "        #add artist\n",
    "        \n",
    "        artist_index = content_to_index[\"ar\"+str(artist_id[0])]\n",
    "        addedartists[artist_index] = 1\n",
    "        \n",
    "        rows[counter] = track_index\n",
    "        cols[counter] = artist_index\n",
    "        val[counter] = 1\n",
    "        counter += 1\n",
    "\n",
    "        #add album\n",
    "        album = album[0].strip(\"[ ]\")\n",
    "\n",
    "        if album != None and len(album) > 0 and not album == \"None\":\n",
    "            album_index = content_to_index[\"al\"+album]\n",
    "            addedalbums[album_index] = 1 #testing\n",
    "            \n",
    "            rows[counter] = track_index\n",
    "            cols[counter] = album_index\n",
    "            val[counter] = 1\n",
    "            counter += 1\n",
    "\n",
    "        #add tags\n",
    "        tags = tags[0].strip('[ ]').split(', ')\n",
    "\n",
    "        for tag in tags: \n",
    "            if len(tag) > 0:\n",
    "                tag = \"t\"+tag\n",
    "                tag_index = content_to_index[tag]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = tag_index\n",
    "                val[counter] = 1\n",
    "                \n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        ## ALT 1: Continuous variables in different content types. \n",
    "        \n",
    "        #add duration\n",
    "        duration = int(duration)\n",
    "        if duration > 0:\n",
    "            duration_bucket = duration_to_bucket(duration)\n",
    "            if duration_bucket > 0:   \n",
    "                duration_index = content_to_index[\"d\"+str(duration_bucket)]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = duration_index\n",
    "\n",
    "                counter+=1\n",
    "        \n",
    "        #add playcount\n",
    "        if playcount is not None and playcount != \"None\" and not math.isnan(playcount):\n",
    "            playcount = int(playcount)\n",
    "            if playcount > 0: \n",
    "                playcount_bucket = playcount_to_bucket(playcount)\n",
    "                playcount_index = content_to_index[\"p\"+str(playcount_bucket)]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = playcount_index\n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ## ALT 2: Continuous variables in one content type. \n",
    "        \n",
    "        #add duration\n",
    "        duration = int(duration)\n",
    "        if duration > 0:\n",
    "            duration_bucket = duration_to_bucket(duration)\n",
    "            duration_index = content_to_index[\"duration\"]\n",
    "\n",
    "            rows[counter] = track_index\n",
    "            cols[counter] = duration_index\n",
    "            val[counter] = duration_bucket/n_duration_buckets\n",
    "            \n",
    "            counter+=1\n",
    "\n",
    "        #add playcount\n",
    "        if playcount is not None and playcount != \"None\" and not math.isnan(playcount):\n",
    "            playcount = int(playcount)\n",
    "            if playcount > 0: \n",
    "                playcount_bucket = playcount_to_bucket(playcount)\n",
    "                playcount_index = content_to_index[\"playcount\"]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = playcount_index\n",
    "                val[counter] = playcount_bucket/n_playcount_buckets\n",
    "\n",
    "                \n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        if trackno%5000 == 0:\n",
    "            print(\"Track %s of %s. %s s sec.\" %(trackno, tracks_matrix.shape[0], round(time.time()-starttime, 2)))  \n",
    "        trackno += 1\n",
    "\n",
    "    #Implicit ratings: all ratings are 1.             \n",
    "    \n",
    "    rows = rows[:counter]\n",
    "    cols = cols[:counter]\n",
    "    val = val[:counter]\n",
    "    #val = np.ones(rows.shape, dtype = int)\n",
    "\n",
    "    #Build ICM matrix. \n",
    "    ICM_all = sps.coo_matrix((val, (rows, cols)), dtype = int)\n",
    "    \n",
    "    print(\"Built ICM matrix with %s content values.\" %(val.shape[0]))\n",
    "    \n",
    "    print(\"%s albums. 27607 expected.\" %len(addedalbums))\n",
    "    print(\"%s artists. 17537 expected.\" %len(addedartists))\n",
    "    \n",
    "    return ICM_all\n",
    "\n",
    "\n",
    "#Build new ICM\n",
    "ICM_all = build_ICM()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "#Get old ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAADuCAYAAACDD21XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a182cf908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def plot_coo_matrix(m):\n",
    "    if not isinstance(m, coo_matrix):\n",
    "        m = coo_matrix(m)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, axisbg='black')\n",
    "    ax.plot(m.col, m.row, 's', color='white', ms=1)\n",
    "    ax.set_xlim(0, m.shape[1])\n",
    "    ax.set_ylim(0, m.shape[0])\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return ax\n",
    "\n",
    "\n",
    "ax = plot_coo_matrix(ICM_all)\n",
    "ax.figure.show()\n",
    "\n",
    "#ax = plot_coo_matrix(d.URM_train)\n",
    "#ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ICM!\n"
     ]
    }
   ],
   "source": [
    "#Save the ICM\n",
    "\n",
    "sps.save_npz(\"Saved Matrixes/ICM_all_coo\", ICM_all)\n",
    "print(\"Saved ICM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted\n"
     ]
    }
   ],
   "source": [
    "#Let's convert to csr. \n",
    "ICM_all = ICM_all.tocsr()\n",
    "print(\"Converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1316175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3091270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id\n",
       "0   1316175\n",
       "1   3885714\n",
       "2   3091270\n",
       "3    226759\n",
       "4    230596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_item_filter(indices):\n",
    "    target_filter = np.zeros((indices), dtype = bool)\n",
    "    for track in target_tracks.values:\n",
    "        track_id = track[0]\n",
    "        track_index = track_to_index[track_id]\n",
    "        target_filter[track_index] = True\n",
    "    print(\"Created filter preserving %s out of %s \" %(np.count_nonzero(target_filter),target_filter.shape[0]))\n",
    "    return target_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.array([1, 2, 3, 4])\n",
    "f = [True, False, True, True]\n",
    "a[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.695766222039\n",
      "  (0, 1)\t0.0\n",
      "  (0, 2)\t0.0\n",
      "  (1, 0)\t0.686244494064\n",
      "  (1, 1)\t0.0\n",
      "  (1, 2)\t0.0\n",
      "  (2, 0)\t0.657064255003\n",
      "  (2, 1)\t0.670560702767\n",
      "  (2, 2)\t0.70811294808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py:274: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using <= is inefficient, try using > instead.\n",
      "  warn(bad_scalar_msg, SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "a = sps.csr_matrix(np.random.rand(3,3))\n",
    "a[0.5 >= a] = 0\n",
    "\n",
    "#print(sps.csr_matrix(a.todense()))\n",
    "print(a)\n",
    "\n",
    "# Vi har en csr.\n",
    "\n",
    "# om vi loopar igenom den och plockar bort noise, sedan skapar ny matrix. \n",
    "\n",
    "\n",
    "#print(sps.csr_matrix(a.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_URM(k = 5): \n",
    "    \n",
    "    playlistList = train_final['playlist_id'].values\n",
    "    itemList = train_final['track_id'].values\n",
    "    \n",
    "    #Translate ids\n",
    "    playlistList_translated = np.zeros(playlistList.shape)\n",
    "    itemList_translated = np.zeros(itemList.shape)\n",
    "    ratingList = np.ones((playlistList.shape), int)\n",
    "    for i in range(train_final.shape[0]):\n",
    "        playlistList_translated[i] = playlist_to_index[playlistList[i]]\n",
    "        itemList_translated[i] = track_to_index[itemList[i]]\n",
    "    \n",
    "    ## Build URM_full. \n",
    "    URM_full = sps.coo_matrix((ratingList, (playlistList_translated, itemList_translated)))\n",
    "    URM_full = URM_train.tocsr()\n",
    "    \n",
    "    ## Build URM_train & URM_test as zeros.\n",
    "    URM_train = URM_full.copy()\n",
    "    URM_test = sps.csr_matrix(np.zeros(URM_full.shape, dtype = int))\n",
    "\n",
    "    # If the data should be splitted. \n",
    "    if k> 0:\n",
    "        ## for each pl\n",
    "        for i, row in enumerate(URM_full): \n",
    "            ## get indexes of tracks\n",
    "            \n",
    "            ## randomly remove k tracks\n",
    "            indices = row.nonzero()[0]\n",
    "            for j in range(2): \n",
    "                removed_index = int(np.floor(np.random.rand()*indices.shape[0]))\n",
    "                removed_track = indices[removed_index]\n",
    "                indices = np.delete(indices,removed_index) #Deletes the int on index removed_index\n",
    "                \n",
    "                #Removes the track from the row\n",
    "                URM_train[i,removed_track] = 0\n",
    "                URM_test[i,removed_track] = 1\n",
    "                \n",
    "    else: \n",
    "        return URM_full, URM_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "row = np.array([1, 0, 1, 0,1,0,1,0])\n",
    "\n",
    "\n",
    "indices = row.nonzero()[0]\n",
    "for j in range(2): \n",
    "    removed_index = int(np.floor(np.random.rand()*indices.shape[0]))\n",
    "    removed_track = indices[removed_index]\n",
    "    indices = np.delete(indices,removed_index) #Deletes the int on index removed_index\n",
    "    row[removed_track] = 0\n",
    "\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7541312, 5550682]\n"
     ]
    }
   ],
   "source": [
    "# Get an owned_by_playlist_dictionary\n",
    "playlist_owned_by = {}\n",
    "for created_at, playlist_id, title, numtracks, duration, owner in playlists_final.as_matrix():\n",
    "    if owner not in playlist_owned_by:\n",
    "        playlist_owned_by[owner] = [playlist_id]\n",
    "    else:\n",
    "        playlist_owned_by[owner].append(playlist_id)\n",
    "        \n",
    "print(playlist_owned_by[40123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 831570. False: 208952. Tot: 1040522\n",
      "Built URM_test\n",
      "Total datapoints: 1040522. Expected: 1040522\n",
      "(57560, 100000)\n",
      "(57560, 100000)\n"
     ]
    }
   ],
   "source": [
    "def build_URM(train_test_split = 0.80):\n",
    "    #Builds urm \n",
    "    \n",
    "    #train_test_split = 1\n",
    "    \n",
    "    numInteractions = train_final.shape[0]\n",
    "\n",
    "    train_mask = np.random.choice(a = [True,False], size = numInteractions, p = [train_test_split, 1-train_test_split])\n",
    "    \n",
    "    playlistList = train_final['playlist_id'].values\n",
    "    itemList = train_final['track_id'].values\n",
    "\n",
    "    #Translate ids\n",
    "    playlistList_translated = np.zeros(playlistList.shape)\n",
    "    itemList_translated = np.zeros(itemList.shape)\n",
    "    ratingList = np.ones((playlistList.shape), int)\n",
    "    \n",
    "    tru = train_mask[train_mask == True].shape[0]\n",
    "    fal = (train_mask[train_mask == False].shape[0])\n",
    "    \n",
    "    print(\"True: %s. False: %s. Tot: %s\" %(tru, fal, (tru+fal)))\n",
    "\n",
    "    \n",
    "    for i in range(train_final.shape[0]):\n",
    "        playlistList_translated[i] = playlist_to_index[playlistList[i]]\n",
    "        itemList_translated[i] = track_to_index[itemList[i]]\n",
    "    #print(\"Translated ids to indexes.\")\n",
    "    \n",
    "    \n",
    "    #Build URM matrix. \n",
    "    URM_train = sps.coo_matrix((ratingList[train_mask], (playlistList_translated[train_mask], itemList_translated[train_mask])))\n",
    "    URM_train = URM_train.tocsr()\n",
    "    #print(\"Built URM_train with shape %s,%s\" %(URM_train.shape[0],URM_train.shape[1]))\n",
    "    \n",
    "    if train_test_split < 1: \n",
    "        #Build URM_test\n",
    "        test_mask = np.logical_not(train_mask)\n",
    "        URM_test = sps.coo_matrix((ratingList[test_mask], (playlistList_translated[test_mask], itemList_translated[test_mask])))\n",
    "        URM_test = URM_test.tocsr()\n",
    "        print(\"Built URM_test\")\n",
    "        testsize = (test_mask[test_mask == True].shape[0])\n",
    "\n",
    "    else: \n",
    "        URM_test = sps.csc_matrix((10, 10), dtype=np.int8)\n",
    "        testsize = 0\n",
    "    \n",
    "    trainsize = train_mask[train_mask == True].shape[0]\n",
    "    totsize = trainsize + testsize\n",
    "    print(\"Total datapoints: %s. Expected: %s\" %(totsize,numInteractions))\n",
    "\n",
    "    \n",
    "    print(URM_train.shape)\n",
    "    print(URM_test.shape)\n",
    "    \n",
    "    return URM_train, URM_test\n",
    "\n",
    "URM_train, URM_test = build_URM(0.8)\n",
    "\n",
    "#Problem: The number of true/false values is not consistent.. Gives problems when testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADWCAYAAADmbvjqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dbf9400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plot_coo_matrix(URM_train)\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 1)\t1\n",
      "[[1 0]\n",
      " [0 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "a = sps.coo_matrix(([1,1,1], ([0, 2, 2], [0, 1, 1])))\n",
    "print(a)\n",
    "print(a.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's take \"secondary\" ratings into account. \n",
    "#start_time = time.time()\n",
    "## For owner in owners\n",
    "#for count, owner in enumerate(playlist_owned_by):\n",
    "#    playlists = playlist_owned_by[owner]\n",
    "#    for i, playlist_id in enumerate(playlists):\n",
    "#        for j, playlist_id in enumerate(playlists): \n",
    "#            if i != j: \n",
    "#                URM_train[i, :] += 0.3 * URM_train[j, :]\n",
    "#    if count % 500 == 0 or count == 50:\n",
    "#        print(\"owned %s of %s. %s sec.\" %(count, len(playlist_owned_by), time.time()-start_time))\n",
    "#\n",
    "## playlist 1 = playlist 1 + 0.3 * playlist 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URM_train contains 831570 interactions. Expected 1040422\n",
      "Train: 0.7978. Test: 0.2022\n"
     ]
    }
   ],
   "source": [
    "#Testing the URM builder.\n",
    "print(\"URM_train contains %s interactions. Expected 1040422\" %URM_train.nnz)\n",
    "testcount = 0\n",
    "traincount = 0\n",
    "itr = 10000\n",
    "for playlist_id, track_id in train_final[0:itr].values: \n",
    "    if (URM_train[playlist_to_index[playlist_id],track_to_index[track_id]]) > 0: \n",
    "        #print(\"Playlist %s with index %s and track %s with index %s was not in URM_train.\" %(playlist_id, playlist_to_index[playlist_id],track_id, track_to_index[track_id]))\n",
    "        traincount += 1\n",
    "    elif (URM_test[playlist_to_index[playlist_id],track_to_index[track_id]]) > 0:\n",
    "        testcount += 1\n",
    "        \n",
    "print(\"Train: %s. Test: %s\"%(traincount/itr, testcount/itr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "class Recommender(object):\n",
    "    def __init__(self, URM, target_items, item_ids, k=50, shrinkage=100, similarity='cosine', filter_method = 'content', topK = 100):\n",
    "        self.dataset = URM\n",
    "        self.target_items = target_items\n",
    "        self.target_item_filter = get_target_item_filter(tracks_final.shape[0])\n",
    "        self.item_ids = item_ids\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        self.filter_method = filter_method\n",
    "        self.topK = topK\n",
    "        \n",
    "        self.UIM = None\n",
    "        \n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'pearson':\n",
    "            self.distance = Pearson(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'adj-cosine':\n",
    "            self.distance = AdjustedCosine(shrinkage=self.shrinkage)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Recommender(similarity={},k={},shrinkage={})\".format(self.similarity_name, self.k, self.shrinkage)\n",
    "\n",
    "    def fit_old(self, X, noise = 0.1):\n",
    "        ## GET ISM MATRIX (I X I)\n",
    "        cp = time.time()\n",
    "        #Calculate cosine similarity    \n",
    "        \n",
    "        ISM = self.distance.compute(X)\n",
    "        print(\"Computed content based similarity matrix. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        \n",
    "        ##GET URM (U X I)\n",
    "        \n",
    "        URM = self.dataset\n",
    "        \n",
    "        ## GET item_ids (1 x I)\n",
    "        \n",
    "        #self.item_ids\n",
    "        \n",
    "        ## FILTER item_ids INTO target_item_ids (1 x tI)\n",
    "        \n",
    "        self.target_item_ids = track_ids[self.target_item_filter]\n",
    "        print(URM.nnz)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## FILTER TARGETED TRACKS\n",
    "        #Maybe this is not working as expected - are we filtering the right tracks? \n",
    "        \n",
    "        # Convert to csc before.. \n",
    "        ISM = ISM[:,self.target_item_filter]\n",
    "        print(\"Filtered targeted tracks in ISM. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        #self.ISM = sps.csr_matrix(self.ISM)\n",
    "        \n",
    "        cp = time.time()  \n",
    "        print(URM.nnz)\n",
    "        #ISM = sps.csr_matrix(ISM)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## CONVERT URM TO CSR\n",
    "        URM = check_matrix(URM, 'csr')\n",
    "        print(\"Checked URM csr %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ##Print dimension\n",
    "        print(URM.shape)\n",
    "        print(ISM.shape)\n",
    "        \n",
    "        ## MULTIPLY URM (U x I) * ISM (I x I)\n",
    "        UIM = URM.dot(ISM)\n",
    "        print(\"Computed URM * ISM %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "        ## MAKE NOT SPARSE\n",
    "        #UIM_dense = UIM.todense()\n",
    "        \n",
    "        ## FILTER UIM into (U x tI) (not needed since I already filtered!)\n",
    "        #UIM_dense = UIM_dense[:,self.target_item_filter]\n",
    "        \n",
    "        ## THIS IS OUR FITTED MODEL\n",
    "        self.UIM = UIM\n",
    "        \n",
    "        return self.UIM\n",
    "  \n",
    "    def fit_new(self, X, noise = 0.1, CF_ratio = 0.5):\n",
    "        ## GET ISM MATRIX (I X I)\n",
    "        cp = time.time()\n",
    "        #Calculate cosine similarity\n",
    "        print(\"Using %s filtering with TopK = %s to compute distance.\" %(self.filter_method, self.topK))\n",
    "        \n",
    "        cosine_cython = Cosine_Similarity(URM_train, TopK=self.topK)\n",
    "        ISM_cf = cosine_cython.compute_similarity()\n",
    "        \n",
    "        print(\"Computed collaborative similarity matrix. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ISM_cont = self.distance.compute(X)\n",
    "        print(\"Computed content based similarity matrix. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        w1 = CF_ratio\n",
    "        w2 = 1-CF_ratio\n",
    "        ISM = w1 * ISM_cf + w2 * ISM_cont\n",
    "        print(\"Combined similarity matrices with ratio %s. %s \" %(CF_ratio, time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ##GET URM (U X I)\n",
    "        \n",
    "        URM = self.dataset\n",
    "        \n",
    "        ## GET item_ids (1 x I)\n",
    "        \n",
    "        #self.item_ids\n",
    "        \n",
    "        ## FILTER item_ids INTO target_item_ids (1 x tI)\n",
    "        \n",
    "        self.target_item_ids = track_ids[self.target_item_filter]\n",
    "        print(URM.nnz)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## FILTER TARGETED TRACKS\n",
    "        #Maybe this is not working as expected - are we filtering the right tracks? \n",
    "        \n",
    "        ISM = ISM[:,self.target_item_filter]\n",
    "        print(\"Filtered targeted tracks in ISM. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        #self.ISM = sps.csr_matrix(self.ISM)\n",
    "        \n",
    "        cp = time.time()  \n",
    "        print(URM.nnz)\n",
    "        #ISM = sps.csr_matrix(ISM)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## CONVERT URM TO CSR\n",
    "        URM = check_matrix(URM, 'csr')\n",
    "        print(\"Checked URM csr %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ##Print dimension\n",
    "        print(URM.shape)\n",
    "        print(ISM.shape)\n",
    "        \n",
    "        ## MULTIPLY URM (U x I) * ISM (I x I)\n",
    "        UIM = URM.dot(ISM)\n",
    "        print(\"Computed URM * ISM %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "        ## MAKE NOT SPARSE\n",
    "        #UIM_dense = UIM.todense()\n",
    "        \n",
    "        ## FILTER UIM into (U x tI) (not needed since I already filtered!)\n",
    "        #UIM_dense = UIM_dense[:,self.target_item_filter]\n",
    "        \n",
    "        ## THIS IS OUR FITTED MODEL\n",
    "        self.UIM = UIM\n",
    "        \n",
    "        return self.UIM\n",
    "\n",
    "        \n",
    "    def recommend_new(self, user_id, at = 5):\n",
    "        ## GET USER_INDEX\n",
    "        user_index = playlist_to_index[user_id]\n",
    "        \n",
    "        # Convert to np.array (why wasn't it before?!)\n",
    "        self.target_item_ids = np.array(self.target_item_ids)\n",
    "        \n",
    "        ## GET ROW CORRESPONDING TO USER (1 x tI)\n",
    "        user_weights = self.UIM[user_index,:].toarray()\n",
    "             \n",
    "        ## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "        top_indexes = np.argsort(user_weights)#[-at:]\n",
    "        top_k_indexes = top_indexes[0, -at:]\n",
    "\n",
    "        ## Translate to indexes\n",
    "        recommendations = self.target_item_ids[top_k_indexes]\n",
    "        \n",
    "        ## RETURN RECOMMENDATIONS\n",
    "        return(recommendations)\n",
    "    \n",
    "    def recommend_dev(self, user_id, at = 5):\n",
    "        print(\"Recommend %s items for user %s!\" %(at, user_id))\n",
    "        ## GET USER_INDEX\n",
    "        user_index = playlist_to_index[user_id]\n",
    "        \n",
    "        # Convert to np.array (why wasn't it before?!)\n",
    "        self.target_item_ids = np.array(self.target_item_ids)\n",
    "        \n",
    "        ## GET ROW CORRESPONDING TO USER (1 x tI)\n",
    "        user_weights = self.UIM[user_index,:].toarray()\n",
    "             \n",
    "        ## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "        top_indexes = np.argsort(user_weights)#[-at:]\n",
    "        print(top_indexes.shape)\n",
    "        top_k_indexes = top_indexes[0, -at:]\n",
    "        print(top_k_indexes.shape)\n",
    "\n",
    "        ## Translate to indexes\n",
    "        recommendations = self.target_item_ids[top_k_indexes]\n",
    "        \n",
    "        ## RETURN RECOMMENDATIONS\n",
    "        return(recommendations)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.dataset[user_id]\n",
    "        print(\"User profile: %s\" %(user_profile))\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "        print(\"Scores: %s\" %(scores))\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(user_id, ranking)\n",
    "        \n",
    "        print(\"Ranking: %s\" %(ranking))\n",
    "        \n",
    "        export = [0,0,0,0,0]\n",
    "        for i in range(5):\n",
    "            t_id = track_to_id[ranking[i]]\n",
    "            export[i] = t_id\n",
    "            \n",
    "        return export\n",
    "    def _filter_seen(self, user_id, ranking):\n",
    "        user_profile = self.dataset[user_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]\n",
    "\n",
    "print(\"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ISimilarity(object):\n",
    "    \"\"\"Abstract interface for the similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, shrinkage=10):\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def compute(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cosine(ISimilarity):\n",
    "    def compute(self, X):\n",
    "        # convert to csc matrix for faster column-wise operations\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "        print(\"Converted to csc.\")\n",
    "        # 1) normalize the columns in X\n",
    "        # compute the column-wise norm\n",
    "        # NOTE: this is slightly inefficient. We must copy X to compute the column norms.\n",
    "        # A faster solution is to  normalize the matrix inplace with a Cython function.\n",
    "        Xsq = X.copy()\n",
    "        Xsq.data **= 2\n",
    "        norm = np.sqrt(Xsq.sum(axis=0))\n",
    "        norm = np.asarray(norm).ravel()\n",
    "        norm += 1e-6\n",
    "        # compute the number of non-zeros in each column\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(X.indptr)\n",
    "        # then normalize the values in each column\n",
    "        X.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "\n",
    "        # 2) compute the cosine similarity using the dot-product\n",
    "        dist = X * X.T\n",
    "        print(\"Computed\")\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        dist = dist - sps.dia_matrix((dist.diagonal()[scipy.newaxis, :], [0]), shape=dist.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            dist = self.apply_shrinkage(X, dist)\n",
    "            print(\"Applied shrinkage\")    \n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n",
    "    \n",
    "    def remove_noise(self, X, noise):\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "        i = 0\n",
    "        for row in X:\n",
    "            r = row\n",
    "            row[row > noise] = 1\n",
    "            row[row <= noise] = 0\n",
    "\n",
    "            X[i,:] = r[row]\n",
    "            i += 1\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kkeep_k_largest(X, k):\n",
    "    \n",
    "    M = X.todense()\n",
    "    for row in M: \n",
    "        top_k_idx = np.argsort(row)\n",
    "        print(row)\n",
    "        print(row[0,top_k_idx[0,-k]])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creating a Item-Item Similarity Matrix based on Collaborative Filtering. \n",
    "class CF(object): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def Item_Similarity(self, URM, k = 100, shrinkage = 10):\n",
    "        #Takes a URM (U x I), returns ISM (I x I)\n",
    "        \n",
    "        self.shrinkage = shrinkage\n",
    "        \n",
    "        # We explore the matrix column-wise\n",
    "        URM = check_matrix(URM, 'csc')\n",
    "\n",
    "        n_items = URM.shape[1]\n",
    "\n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        processedItems = 0\n",
    "\n",
    "        # Compute all similarities for each item using vectorization\n",
    "        for itemIndex in range(n_items):\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % 100==0:\n",
    "\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "            # All ratings for a given item\n",
    "            item_ratings = URM[:,itemIndex]\n",
    "            item_ratings = item_ratings.toarray().squeeze()\n",
    "            #print(item_ratings)\n",
    "\n",
    "            # Compute item similarities\n",
    "            this_item_weights = URM.T.dot(item_ratings)\n",
    "            \n",
    "            #print(this_item_weights)\n",
    "\n",
    "            # Sort indices and select TopK\n",
    "            top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "            # Incrementally build sparse matrix\n",
    "            #print(top_k_idx)\n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(URM.shape[1])[top_k_idx])\n",
    "            cols.extend(np.ones(k) * itemIndex)\n",
    "          \n",
    "            \n",
    "        W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                                shape=(n_items, n_items),\n",
    "                                dtype=np.float32)\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        W_sparse = W_sparse - sps.dia_matrix((W_sparse.diagonal()[scipy.newaxis, :], [0]), shape=W_sparse.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        W_sparse.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if shrinkage > 0:\n",
    "            W_sparse = self.apply_shrinkage(URM, W_sparse)\n",
    "            print(\"Applied shrinkage\") \n",
    "\n",
    "        return W_sparse\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 1]\n",
      " [0 0 1 0 1 1]\n",
      " [1 0 1 0 1 0]]\n",
      "Removed diagonal\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-11a0d91352a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mISM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItem_Similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURM_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrinkage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mISM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mISM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-d33ee41652a7>\u001b[0m in \u001b[0;36mItem_Similarity\u001b[0;34m(self, URM, k, shrinkage)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Removed diagonal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mW_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_nnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Normalized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm' is not defined"
     ]
    }
   ],
   "source": [
    "URM = np.matrix([[1, 0, 1, 0, 0, 1], [0, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0]])\n",
    "print(URM)\n",
    "cf = CF()\n",
    "URM_sparse = sps.csc_matrix(URM, dtype = int)\n",
    "\n",
    "\n",
    "ISM = cf.Item_Similarity(URM_sparse, k = 4, shrinkage = 0)\n",
    "print(ISM.nnz)\n",
    "print(ISM.todense())\n",
    "print(ISM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "cdef class Cosine_Similarity:\n",
    "\n",
    "    cdef int TopK\n",
    "    cdef long n_items\n",
    "\n",
    "    # Arrays containing the sparse data\n",
    "    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n",
    "    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n",
    "    cdef double[:] user_to_item_data, item_to_user_data\n",
    "\n",
    "    # In case you select no TopK\n",
    "    cdef double[:,:] W_dense\n",
    "\n",
    "    \n",
    "    def __init__(self, URM, TopK = 100):\n",
    "        \"\"\"\n",
    "        Dataset must be a matrix with items as columns\n",
    "        :param dataset:\n",
    "        :param TopK:\n",
    "        \"\"\"\n",
    "\n",
    "        super(Cosine_Similarity, self).__init__()\n",
    "\n",
    "        self.n_items = URM.shape[1]\n",
    "\n",
    "        self.TopK = min(TopK, self.n_items)\n",
    "\n",
    "        URM = URM.tocsr()\n",
    "        self.user_to_item_row_ptr = URM.indptr\n",
    "        self.user_to_item_cols = URM.indices\n",
    "        self.user_to_item_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        URM = URM.tocsc()\n",
    "        self.item_to_user_rows = URM.indices\n",
    "        self.item_to_user_col_ptr = URM.indptr\n",
    "        self.item_to_user_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_items,self.n_items))\n",
    "\n",
    "\n",
    "\n",
    "    cdef int[:] getUsersThatRatedItem(self, long item_id):\n",
    "        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n",
    "\n",
    "    cdef int[:] getItemsRatedByUser(self, long user_id):\n",
    "        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n",
    "\n",
    "    \n",
    "    \n",
    "    cdef double[:] computeItemSimilarities(self, long item_id_input):\n",
    "        \"\"\"\n",
    "        For every item the cosine similarity against other items depends on whether they have users in common. \n",
    "        The more common users the higher the similarity.\n",
    "        \n",
    "        The basic implementation is:\n",
    "        - Select the first item\n",
    "        - Loop through all other items\n",
    "        -- Given the two items, get the users they have in common\n",
    "        -- Update the similarity considering all common users\n",
    "        \n",
    "        That is VERY slow due to the common user part, in which a long data structure is looped multiple times.\n",
    "        \n",
    "        A better way is to use the data structure in a different way skipping the search part, getting directly\n",
    "        the information we need.\n",
    "        \n",
    "        The implementation here used is:\n",
    "        - Select the first item\n",
    "        - Initialize a zero valued array for the similarities\n",
    "        - Get the users who rated the first item\n",
    "        - Loop through the users\n",
    "        -- Given a user, get the items he rated (second item)\n",
    "        -- Update the similarity of the items he rated\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create template used to initialize an array with zeros\n",
    "        # Much faster than np.zeros(self.n_items)\n",
    "        cdef array[double] template_zero = array('d')\n",
    "        cdef array[double] result = clone(template_zero, self.n_items, zero=True)\n",
    "\n",
    "\n",
    "        cdef long user_index, user_id, item_index, item_id_second\n",
    "\n",
    "        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n",
    "        cdef int[:] items_rated_by_user\n",
    "\n",
    "        cdef double rating_item_input, rating_item_second\n",
    "\n",
    "        # Get users that rated the items\n",
    "        for user_index in range(len(users_that_rated_item)):\n",
    "\n",
    "            user_id = users_that_rated_item[user_index]\n",
    "            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n",
    "\n",
    "            # Get all items rated by that user\n",
    "            items_rated_by_user = self.getItemsRatedByUser(user_id)\n",
    "\n",
    "            for item_index in range(len(items_rated_by_user)):\n",
    "\n",
    "                item_id_second = items_rated_by_user[item_index]\n",
    "\n",
    "                # Do not compute the similarity on the diagonal\n",
    "                if item_id_second != item_id_input:\n",
    "                    # Increment similairty\n",
    "                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n",
    "\n",
    "                    result[item_id_second] += rating_item_input*rating_item_second\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def compute_similarity(self):\n",
    "\n",
    "        cdef int itemIndex, innerItemIndex\n",
    "        cdef long long topKItemIndex\n",
    "\n",
    "        cdef long long[:] top_k_idx\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\n",
    "\n",
    "        #cdef long[:] top_k_idx\n",
    "        cdef double[:] this_item_weights\n",
    "\n",
    "        cdef long processedItems = 0\n",
    "\n",
    "        # Data structure to incrementally build sparse matrix\n",
    "        # Preinitialize max possible length\n",
    "        cdef double[:] values = np.zeros((self.n_items*self.TopK))\n",
    "        cdef int[:] rows = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef int[:] cols = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef long sparse_data_pointer = 0\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute all similarities for each item\n",
    "        for itemIndex in range(self.n_items):\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % 10000==0 or processedItems==self.n_items:\n",
    "\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print(\"Similarity item {} ( {:2.0f} % ), {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, processedItems*1.0/self.n_items*100, itemPerSec, (self.n_items-processedItems) / itemPerSec / 60))\n",
    "\n",
    "            this_item_weights = self.computeItemSimilarities(itemIndex)\n",
    "\n",
    "            if self.TopK == 0:\n",
    "\n",
    "                for innerItemIndex in range(self.n_items):\n",
    "                    self.W_dense[innerItemIndex,itemIndex] = this_item_weights[innerItemIndex]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                # top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "\n",
    "                this_item_weights_np = - np.array(this_item_weights)\n",
    "                \n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(this_item_weights_np, self.TopK-1)[0:self.TopK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "\n",
    "\n",
    "                # Incrementally build sparse matrix\n",
    "                for innerItemIndex in range(len(top_k_idx)):\n",
    "\n",
    "                    topKItemIndex = top_k_idx[innerItemIndex]\n",
    "\n",
    "                    values[sparse_data_pointer] = this_item_weights[topKItemIndex]\n",
    "                    rows[sparse_data_pointer] = topKItemIndex\n",
    "                    cols[sparse_data_pointer] = itemIndex\n",
    "\n",
    "                    sparse_data_pointer += 1\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "\n",
    "            return np.array(self.W_dense)\n",
    "\n",
    "        else:\n",
    "\n",
    "            values = np.array(values[0:sparse_data_pointer])\n",
    "            rows = np.array(rows[0:sparse_data_pointer])\n",
    "            cols = np.array(cols[0:sparse_data_pointer])\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                    shape=(self.n_items, self.n_items),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#URM_train, URM_test = build_URM(0.8)\n",
    "\n",
    "#cosine_cython = Cosine_Similarity(URM_train, TopK=50)\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "#ISM = cosine_cython.compute_similarity()\n",
    "\n",
    "#print(\"Similarity computed in {:.2f} seconds\".format(time.time()-start_time))\n",
    "#print(ISM.shape)\n",
    "#print(ISM.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with train_rate 0.8\n",
      "True: 832207. False: 208315. Tot: 1040522\n",
      "Built URM_test\n",
      "Total datapoints: 1040522. Expected: 1040522\n",
      "(57560, 100000)\n",
      "(57560, 100000)\n",
      "Created filter preserving 32195 out of 100000 \n",
      "Loaded ICM!\n",
      "Converted to csc.\n",
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Computed content based similarity matrix. 269.0309388637543 \n",
      "832207\n",
      "2228161232\n",
      "Filtered targeted tracks in ISM. 95.64251804351807 \n",
      "832207\n",
      "758173235\n",
      "Checked URM csr 0.04882526397705078 \n",
      "(57560, 100000)\n",
      "(100000, 32195)\n",
      "Computed URM * ISM 97.96644902229309 \n",
      "Model fitted in 463.38104581832886 seconds\n"
     ]
    }
   ],
   "source": [
    "### This is the main script ###\n",
    "\n",
    "\n",
    "#1. Fitting the model. \n",
    "\n",
    "#If export is true, the recommendations will be written to file. \n",
    "#If false, evaluation method can be used. \n",
    "\n",
    "## PARAMS\n",
    "\n",
    "export = True\n",
    "filter_method = 'content'\n",
    "topK = 100\n",
    "shrinkage=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if export:\n",
    "    train_rate = 1\n",
    "else:\n",
    "    train_rate = 0.8\n",
    "    \n",
    "print(\"Running with train_rate %s\" %(train_rate))\n",
    "\n",
    "URM_train, URM_test = build_URM(train_rate)\n",
    "\n",
    "import time\n",
    "starttime = time.time()\n",
    "rec = Recommender(URM=URM_train, \n",
    "                  target_items = target_tracks, \n",
    "                  item_ids = track_ids, \n",
    "                  shrinkage = shrinkage, \n",
    "                  filter_method = filter_method,\n",
    "                  topK = topK)\n",
    "\n",
    "ICM_all = sps.load_npz(\"Saved Matrixes/ICM_perfect.npz\")\n",
    "print(\"Loaded ICM!\")\n",
    "\n",
    "#ax = plot_coo_matrix(ICM_all)\n",
    "#ax.figure.show()\n",
    "\n",
    "rec.fit_old(ICM_all)\n",
    "\n",
    "print(\"Model fitted in %s seconds\" %(time.time()-starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10000 playlists, 0.0024776458740234375 sec.\n",
      "1000 out of 10000 playlists, 2.3530449867248535 sec.\n",
      "2000 out of 10000 playlists, 4.595810651779175 sec.\n",
      "3000 out of 10000 playlists, 6.758625030517578 sec.\n",
      "4000 out of 10000 playlists, 8.847862005233765 sec.\n",
      "5000 out of 10000 playlists, 10.919148683547974 sec.\n",
      "6000 out of 10000 playlists, 12.996845722198486 sec.\n",
      "7000 out of 10000 playlists, 15.06330680847168 sec.\n",
      "8000 out of 10000 playlists, 17.141454696655273 sec.\n",
      "9000 out of 10000 playlists, 19.20903968811035 sec.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#2. Creating recommendations. \n",
    "zeros = np.zeros((target_playlists.size, 6), dtype = int)\n",
    "recommendations = pd.DataFrame(zeros)\n",
    "recommendations.columns = ['playlist_id', 1, 2, 3, 4, 5]\n",
    "\n",
    "counter = 0\n",
    "starttime = time.time()\n",
    "for playlist_id in target_playlists['playlist_id']:\n",
    "\n",
    "    if counter % 1000 == 0: \n",
    "        print (\"%s out of 10000 playlists, %s sec.\" %(counter, time.time()-starttime))\n",
    "\n",
    "    playlist_id_translated = playlist_to_index[int(playlist_id)]\n",
    "    recommendations.iloc[counter, 1:6] = rec.recommend_new(playlist_id, 5)\n",
    "    recommendations.iloc[counter, 0] = playlist_id\n",
    "    counter += 1\n",
    "\n",
    "if export:\n",
    "    filename = \"recommendations_6/11_\"\n",
    "    np.savetxt(\"output/rec_%s_k_%s_shrink_%s.csv\" %(filter_method, topK, shrinkage),recommendations, fmt = '%s,%s %s %s %s %s', header = \"playlist_id,track_ids\", newline = \"\\n\")\n",
    "    print(\"Saved to file \")\n",
    "\n",
    "#print(recommendations)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57560, 100000)\n",
      "(57560, 100000)\n"
     ]
    }
   ],
   "source": [
    "# 3. Want to evaluate? \n",
    "print(URM_test.shape)\n",
    "print(URM_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender performance is: Precision = 0.0355, Recall = 0.0309, MAP = 0.0218\n"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm(URM_test, recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 out of 50000 were in the target.\n"
     ]
    }
   ],
   "source": [
    "# Does the recommender rec just targeted tracks? \n",
    "def test_all_rec_in_target(recommendations):\n",
    "    tt = target_tracks.values\n",
    "    recommendations = recommendations.as_matrix()\n",
    "    notcount = 0\n",
    "    count = 0\n",
    "    for row in recommendations: \n",
    "        for item in row[1:6]: \n",
    "            count += 1\n",
    "            if item in tt: \n",
    "                notcount += 1\n",
    "                #print(\"Rec not in target! %s\" %item)\n",
    "    print(\"%s out of %s were in the target.\" %(notcount, count))\n",
    "    \n",
    "test_all_rec_in_target(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TESTING THE REC FUNCTION - SHOULD WORK\n",
    "#Fitted in 172.8 seconds\n",
    "\n",
    "rec_dev = Recommender(URM=URM_train, target_items = target_tracks, item_ids = track_ids, shrinkage=0.0)\n",
    "rec_dev.UIM = rec.UIM\n",
    "rec_dev.target_item_ids = rec.target_item_ids\n",
    "\n",
    "zeros = np.zeros((1, 6), dtype = int)\n",
    "recommendations = pd.DataFrame(zeros)\n",
    "recommendations.columns = ['playlist_id', 1, 2, 3, 4, 5]\n",
    "recommendations.iloc[counter, 1:6] = rec_dev.recommend_dev(playlist_to_id[30680], 5)\n",
    "recommendations.iloc[counter, 0] = playlist_to_id[30680]\n",
    "\n",
    "print(recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "rec = Recommender(URM=URM_train, shrinkage=0.0)\n",
    "rec.fit(ICM_all)\n",
    "print(\"Done in %s seconds\" %(time.time()-starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ICM_add_IDF(ICM): \n",
    "    num_tot_items = ICM_all.shape[0]\n",
    "\n",
    "    # let's count how many items have a certain feature\n",
    "    items_per_feature = (ICM_all > 0).sum(axis=0)\n",
    "\n",
    "    IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "\n",
    "    print(ICM_all.shape)\n",
    "    print(IDF.shape)\n",
    "    ICM_idf = sps.csr_matrix(ICM_all, dtype=np.float64)\n",
    "    # compute the number of non-zeros in each col\n",
    "    # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "    col_nnz = np.diff(check_matrix(ICM_idf, 'csc').indptr)\n",
    "    print(col_nnz.shape)\n",
    "    print(ICM_idf.shape)\n",
    "    print(IDF.shape)\n",
    "    # then normalize the values in each col\n",
    "    ICM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "    return ICM_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_idf = BasicItemKNNRecommender(URM=URM_train, shrinkage=0.0, k=50)\n",
    "rec_idf.fit(ICM_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation functions\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommendations, at=5):\n",
    "    \n",
    "    starttime = time.time()\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "    \n",
    "    playlists = target_playlists['playlist_id']\n",
    "    \n",
    "    for i, playlist_id in enumerate(playlists):\n",
    "        relevant_items = URM_test[playlist_to_index[playlist_id]].indices\n",
    "        \n",
    "        for j, item_id in enumerate(relevant_items):\n",
    "            relevant_items[j] = track_to_id[item_id]\n",
    "        \n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommendations.iloc[i,1:6]\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_file():\n",
    "    #Saves the recommendations dataframe to the .csv-file. \n",
    "    np.savetxt(\"output/recommendations_more_content.csv\",recommendations, fmt = '%s,%s %s %s %s %s', header = \"playlist_id,track_ids\", newline = \"\\n\")\n",
    "    \n",
    "    \n",
    "def test():\n",
    "    #Do something\n",
    "    print(\"Result: \")\n",
    "    pass\n",
    "\n",
    "\n",
    "save_to_file()\n",
    "print(recommendations.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "ax = plot_coo_matrix(rec.UIM)\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
