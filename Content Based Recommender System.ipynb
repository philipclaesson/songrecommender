{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sps\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "#train_final.csv - the training set of interactions\n",
    "train_final = pd.read_csv('input/train_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#tracks_final.csv - supplementary information about the items\n",
    "tracks_final = pd.read_csv('input/tracks_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#playlists_final.csv - supplementary information about the users\n",
    "playlists_final = pd.read_csv('input/playlists_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#target_playlists.csv - the set of target playlists that will receive recommendations\n",
    "target_playlists = pd.read_csv('input/target_playlists.csv');\n",
    "\n",
    "#target_tracks.csv - the set of target items (tracks) to be recommended\n",
    "target_tracks = pd.read_csv('input/target_tracks.csv');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_final now contains 806461 interactions. \n",
      "Tracks_final now contains 50269 tracks. \n"
     ]
    }
   ],
   "source": [
    "#This step is not needed yet, will make ratings worse! \n",
    "\n",
    "def get_relevant_tracks():\n",
    "    #Now we want to remove some redundant stuff. \n",
    "\n",
    "    #We will remove all songs which are not occurring more than 10 times in train_final\n",
    "    #Nevertheless, we still want to keep all tracks which are in the target tracks.  \n",
    "\n",
    "    popularity = train_final.groupby(by=\"track_id\").playlist_id.nunique().to_frame()\n",
    "\n",
    "    #remove index name\n",
    "    popularity.reset_index(level = 0, inplace = True)\n",
    "\n",
    "    #Rename the columns\n",
    "    popularity.columns = ['track_id','occurrences']\n",
    "\n",
    "    #Remove all targeted tracks - TESTED, working as expected\n",
    "    tracks_relevant = popularity[~popularity['track_id'].isin(target_tracks['track_id'])]\n",
    "\n",
    "    #Remove tracks occurring less than 10 times\n",
    "    tracks_relevant = tracks_relevant[tracks_relevant['occurrences'] > 7]\n",
    "\n",
    "    #Add the targeteted tracks back again\n",
    "    tracks_relevant = pd.concat([tracks_relevant, target_tracks])\n",
    "\n",
    "    return(tracks_relevant)\n",
    "\n",
    "    print(\"Removed %s redundant tracks which occured less than 10 times.\" %(tracks_final-tracks_relevant))\n",
    "\n",
    "tracks_relevant = get_relevant_tracks()\n",
    "\n",
    "#Remove irrelevant tracks from train_final and tracks_final\n",
    "train_final = train_final[train_final['track_id'].isin(tracks_relevant['track_id'])]\n",
    "\n",
    "print(\"Train_final now contains %s interactions. \" %(train_final.shape[0]))\n",
    "\n",
    "tracks_final = tracks_final[tracks_final['track_id'].isin(tracks_relevant['track_id'])]\n",
    "\n",
    "print(\"Tracks_final now contains %s tracks. \"%(tracks_final.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>playcount</th>\n",
       "      <th>album</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2972914</td>\n",
       "      <td>144</td>\n",
       "      <td>224000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[54087, 1757, 1718, 116712, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750239</td>\n",
       "      <td>246</td>\n",
       "      <td>157000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[189631, 3424, 177424, 46208, 205245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550729</td>\n",
       "      <td>144</td>\n",
       "      <td>217000</td>\n",
       "      <td>554.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 46869, 183258, 54337]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2169950</td>\n",
       "      <td>144</td>\n",
       "      <td>207000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 70618, 207003, 109806, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1373492</td>\n",
       "      <td>928</td>\n",
       "      <td>237000</td>\n",
       "      <td>2896.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[81223, 11056, 205245, 189631, 3982]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_id  artist_id  duration  playcount album  \\\n",
       "0    2972914        144    224000       49.0   [7]   \n",
       "1    2750239        246    157000        1.0   [8]   \n",
       "2    1550729        144    217000      554.0   [9]   \n",
       "3    2169950        144    207000      200.0   [9]   \n",
       "10   1373492        928    237000     2896.0  [22]   \n",
       "\n",
       "                                      tags  \n",
       "0      [54087, 1757, 1718, 116712, 189631]  \n",
       "1    [189631, 3424, 177424, 46208, 205245]  \n",
       "2    [54087, 109806, 46869, 183258, 54337]  \n",
       "3   [54087, 70618, 207003, 109806, 116712]  \n",
       "10    [81223, 11056, 205245, 189631, 3982]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets take a look at the tags.\n",
    "tracks_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Translating all content ids into indexes.\n",
    "\n",
    "#We need to create buckets for the playcount and duration. \n",
    "#Lets create buckets and a help function for the duration. \n",
    "\n",
    "n_duration_buckets = 3\n",
    "def duration_to_bucket(duration, alternative = 2):\n",
    "    if (alternative == 1):\n",
    "        n_duration_buckets = 8\n",
    "        if duration <= 0:\n",
    "            print(\"Null duration reached bucket function. \")\n",
    "            return None\n",
    "        elif duration < 90000: #not a song\n",
    "            return 1\n",
    "        elif duration < 140000: #short song\n",
    "            return 2\n",
    "        elif duration < 220000: #radio song\n",
    "            return 3\n",
    "        elif duration < 340000: #normal song\n",
    "            return 4\n",
    "        elif duration < 480000: #long song\n",
    "            return 5\n",
    "        elif duration < 720000: #really long\n",
    "            return 6\n",
    "        elif duration < 1200000: #super long\n",
    "            return 7\n",
    "        elif duration >= 1200000: #mixtape/compilation\n",
    "            return 8\n",
    "    elif(alternative == 2):\n",
    "        n_duration_buckets = 3\n",
    "        if duration <= 0:\n",
    "            print(\"Null duration reached bucket function. \")\n",
    "            return None\n",
    "        elif duration <= 150000: #very short\n",
    "            return 1\n",
    "        elif duration > 150000 and duration < 720000: #very long\n",
    "            return 2\n",
    "        elif duration >= 720000: #mixtape/compilation\n",
    "            return 3\n",
    "        \n",
    "\n",
    "n_playcount_buckets = 7\n",
    "def playcount_to_bucket(playcount):\n",
    "    if playcount <= 0 or playcount is None:\n",
    "        print(\"Null playcount reached bucket function. \")\n",
    "        return None\n",
    "    elif playcount < 254: #0,4 percentile not popular\n",
    "        return 1\n",
    "    elif playcount < 881: #0,6 perc: known\n",
    "        return 2\n",
    "    elif playcount < 1560: #0,7 popular\n",
    "        return 3\n",
    "    elif playcount < 2808: #0,8 very popular\n",
    "        return 4\n",
    "    elif playcount < 5900: #0,9 hits\n",
    "        return 5\n",
    "    elif playcount < 10494: #0,95 super hits\n",
    "        return 6\n",
    "    elif playcount >= 10494: # mega hits\n",
    "        return 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added d1\n",
      "added d2\n",
      "added d3\n",
      "42489\n"
     ]
    }
   ],
   "source": [
    "tracks_final['tags'].head()\n",
    "\n",
    "content_to_index = {}\n",
    "content_to_id = {}\n",
    "content_counter = 0\n",
    "\n",
    "#Lets translate the tags to indexes. \n",
    "for row in tracks_final['tags']:\n",
    "    tags = row.strip('[ ]').split(', ')\n",
    "    for tag in tags:\n",
    "        if len(tag) > 0: \n",
    "            tag = \"t\"+tag\n",
    "            if not(tag in content_to_index):\n",
    "                content_to_index[tag] = content_counter\n",
    "                content_to_id[content_counter] = tag\n",
    "                content_counter += 1;\n",
    "                \n",
    "#Lets translate album into indexes\n",
    "for album in tracks_final['album']:\n",
    "    album = album.strip('[ ]')\n",
    "    if album != None and len(album) > 0: #None should not be considered content\n",
    "        album = \"al\"+album\n",
    "        if not(album in content_to_index):\n",
    "            content_to_index[album] = content_counter\n",
    "            content_to_id[content_counter] = album\n",
    "            content_counter += 1;\n",
    "\n",
    "#Lets translate artist_id into indexes\n",
    "for artist in tracks_final['artist_id']:\n",
    "    artist = str(artist)\n",
    "    artist = \"ar\"+artist\n",
    "    if not(artist in content_to_index):\n",
    "        content_to_index[artist] = content_counter\n",
    "        content_to_id[content_counter] = artist\n",
    "        content_counter += 1;\n",
    "        \n",
    "\n",
    "#Lets translate the duration buckets into indexes. \n",
    "for bucket in range(n_duration_buckets): \n",
    "    bucket = \"d\"+str(bucket+1)\n",
    "    content_to_index[bucket] = content_counter\n",
    "    content_to_id[content_counter] = bucket\n",
    "    print(\"added %s\" %(bucket))\n",
    "    content_counter += 1\n",
    "\"\"\"\n",
    "#Lets translate the playcount buckets into indexes. \n",
    "for playcount in range(n_playcount_buckets): \n",
    "    playcount = \"p\"+str(playcount+1)\n",
    "    content_to_index[playcount] = content_counter\n",
    "    content_to_id[content_counter] = playcount\n",
    "    \n",
    "    content_counter += 1\n",
    "\n",
    "\n",
    "## Alternative 2: Just one content type per continous variable. \n",
    "#Fun thing to try: can I add all duration/playcounts in one col, normalizing from 0-1? \n",
    "\n",
    "\n",
    "content_to_index[\"duration\"] = content_counter\n",
    "content_to_id[content_counter] = \"duration\"\n",
    "content_counter += 1\n",
    "\n",
    "content_to_index[\"playcount\"] = content_counter\n",
    "content_to_id[content_counter] = \"playcount\"\n",
    "content_counter += 1\n",
    "\"\"\"\n",
    "\n",
    "print(len(content_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 57561 playlists with 50269 unique tracks with 42489 unique content types. \n"
     ]
    }
   ],
   "source": [
    "#If we translate each track_id to a track_index which will serve as matrix index, we can save a lot of time. \n",
    "\n",
    "\n",
    "#We need a way to get from track_id to index in O(1).\n",
    "#Let's create a dictionary\n",
    "\n",
    "track_to_id = {}\n",
    "track_to_index = {}\n",
    "track_ids = tracks_final['track_id']\n",
    "\n",
    "counter = 0; #We will start at 1, reserving col 0 for indexes.  \n",
    "for track_id in tracks_final['track_id']:\n",
    "    track_id = int(track_id)\n",
    "    track_to_index[track_id] = counter\n",
    "    track_to_id[counter] = track_id\n",
    "    counter += 1;\n",
    "    \n",
    "#and a way to get from playlist_id to index in O(1)\n",
    "\n",
    "\n",
    "playlist_to_index = {}\n",
    "playlist_to_id = {}\n",
    "counter = 0; \n",
    "for playlist_id in playlists_final['playlist_id']:\n",
    "    playlist_id = int(playlist_id)\n",
    "    playlist_to_index[playlist_id] = counter\n",
    "    playlist_to_id[counter] = playlist_id\n",
    "    counter += 1;\n",
    "    \n",
    "print(\"We have {} playlists with {} unique tracks with {} unique content types. \".format(len(playlist_to_index), len(track_to_index), len(content_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can create an Item Content Matrix. \n",
    "\n",
    "#ICM_all = np.zeros((len(tracks_indexes), len(tags_indexes)), int)\n",
    "#ICM_all = sps.coo_matrix((len(track_to_index), len(content_to_index)), int)\n",
    "#print(ICM_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>playcount</th>\n",
       "      <th>album</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2972914</td>\n",
       "      <td>144</td>\n",
       "      <td>224000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[54087, 1757, 1718, 116712, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750239</td>\n",
       "      <td>246</td>\n",
       "      <td>157000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[189631, 3424, 177424, 46208, 205245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550729</td>\n",
       "      <td>144</td>\n",
       "      <td>217000</td>\n",
       "      <td>554.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 46869, 183258, 54337]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2169950</td>\n",
       "      <td>144</td>\n",
       "      <td>207000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 70618, 207003, 109806, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1373492</td>\n",
       "      <td>928</td>\n",
       "      <td>237000</td>\n",
       "      <td>2896.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[81223, 11056, 205245, 189631, 3982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3387498</td>\n",
       "      <td>928</td>\n",
       "      <td>245000</td>\n",
       "      <td>9622.0</td>\n",
       "      <td>[31]</td>\n",
       "      <td>[81223, 189631, 205245, 4425, 50764]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>699524</td>\n",
       "      <td>928</td>\n",
       "      <td>294000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>[26]</td>\n",
       "      <td>[11056, 205245, 81223, 4425, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>47604</td>\n",
       "      <td>943</td>\n",
       "      <td>274000</td>\n",
       "      <td>299.0</td>\n",
       "      <td>[38]</td>\n",
       "      <td>[116047, 116198, 9916, 157591, 116155]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2721759</td>\n",
       "      <td>943</td>\n",
       "      <td>398000</td>\n",
       "      <td>2069.0</td>\n",
       "      <td>[39]</td>\n",
       "      <td>[116047, 50604, 116155, 50764, 154891]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3521362</td>\n",
       "      <td>1115</td>\n",
       "      <td>359000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>[105]</td>\n",
       "      <td>[205245, 3668, 189631, 404, 46208]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_id  artist_id  duration  playcount  album  \\\n",
       "0    2972914        144    224000       49.0    [7]   \n",
       "1    2750239        246    157000        1.0    [8]   \n",
       "2    1550729        144    217000      554.0    [9]   \n",
       "3    2169950        144    207000      200.0    [9]   \n",
       "10   1373492        928    237000     2896.0   [22]   \n",
       "11   3387498        928    245000     9622.0   [31]   \n",
       "12    699524        928    294000       22.0   [26]   \n",
       "14     47604        943    274000      299.0   [38]   \n",
       "15   2721759        943    398000     2069.0   [39]   \n",
       "18   3521362       1115    359000       46.0  [105]   \n",
       "\n",
       "                                      tags  \n",
       "0      [54087, 1757, 1718, 116712, 189631]  \n",
       "1    [189631, 3424, 177424, 46208, 205245]  \n",
       "2    [54087, 109806, 46869, 183258, 54337]  \n",
       "3   [54087, 70618, 207003, 109806, 116712]  \n",
       "10    [81223, 11056, 205245, 189631, 3982]  \n",
       "11    [81223, 189631, 205245, 4425, 50764]  \n",
       "12    [11056, 205245, 81223, 4425, 189631]  \n",
       "14  [116047, 116198, 9916, 157591, 116155]  \n",
       "15  [116047, 50604, 116155, 50764, 154891]  \n",
       "18      [205245, 3668, 189631, 404, 46208]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 0 of 50269. 0.0 s sec.\n",
      "Track 5000 of 50269. 0.11 s sec.\n",
      "Track 10000 of 50269. 0.22 s sec.\n",
      "Track 15000 of 50269. 0.34 s sec.\n",
      "Track 20000 of 50269. 0.47 s sec.\n",
      "Track 25000 of 50269. 0.59 s sec.\n",
      "Track 30000 of 50269. 0.7 s sec.\n",
      "Track 35000 of 50269. 0.82 s sec.\n",
      "Track 40000 of 50269. 0.94 s sec.\n",
      "Track 45000 of 50269. 1.06 s sec.\n",
      "Track 50000 of 50269. 1.17 s sec.\n",
      "Built ICM matrix with 382324 content values.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#So let's fill the ICM with our data.\n",
    "import math\n",
    "\n",
    "def build_ICM():\n",
    "    \n",
    "    no_interactions = 1000000\n",
    "    \n",
    "    tracks_matrix = tracks_final.as_matrix()\n",
    "    rows = np.zeros((no_interactions,), dtype = int)\n",
    "    cols = np.zeros((no_interactions,), dtype = int)\n",
    "    val = np.zeros((no_interactions,), dtype = int)\n",
    "    #val[i] = value of row[i] col[i]\n",
    "    #val = []\n",
    "    counter = 0\n",
    "    starttime = time.time()\n",
    "    lasttime = starttime\n",
    "    trackno = 0\n",
    "        \n",
    "    for track in tracks_matrix: \n",
    "        track_id, artist_id, duration, playcount, album, tags = np.split(track, 6)\n",
    "\n",
    "        #Get track index\n",
    "        track_index = track_to_index[int(track_id[0])]\n",
    "\n",
    "\n",
    "        #add artist\n",
    "        artist_index = content_to_index[\"ar\"+str(artist_id[0])]\n",
    "\n",
    "        rows[counter] = track_index\n",
    "        cols[counter] = artist_index\n",
    "        val[counter] = 1\n",
    "        counter += 1\n",
    "\n",
    "        #add album\n",
    "        album = album[0].strip(\"[ ]\")\n",
    "        if album != None and len(album) > 0 and not album == (\"None\"):\n",
    "            album_index = content_to_index[\"al\"+album]\n",
    "\n",
    "            rows[counter] = track_index\n",
    "            cols[counter] = album_index\n",
    "            val[counter] = 1\n",
    "            counter += 1\n",
    "\n",
    "        #add tags\n",
    "        tags = tags[0].strip('[ ]').split(', ')\n",
    "\n",
    "        for tag in tags: \n",
    "            if len(tag) > 0:\n",
    "                tag = \"t\"+tag\n",
    "                tag_index = content_to_index[tag]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = tag_index\n",
    "                val[counter] = 1\n",
    "                \n",
    "                counter+=1\n",
    "        \n",
    "        ## ALT 1: Continuous variables in different content types. \n",
    "        \n",
    "        #add duration\n",
    "        duration = int(duration)\n",
    "        if duration > 0:\n",
    "            duration_bucket = duration_to_bucket(duration)\n",
    "            duration_index = content_to_index[\"d\"+str(duration_bucket)]\n",
    "\n",
    "            rows[counter] = track_index\n",
    "            cols[counter] = duration_index\n",
    "            \n",
    "            counter+=1\n",
    "        \"\"\"\n",
    "        #add playcount\n",
    "        if playcount is not None and playcount != \"None\" and not math.isnan(playcount):\n",
    "            playcount = int(playcount)\n",
    "            if playcount > 0: \n",
    "                playcount_bucket = playcount_to_bucket(playcount)\n",
    "                playcount_index = content_to_index[\"p\"+str(playcount_bucket)]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = playcount_index\n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ## ALT 2: Continuous variables in one content type. \n",
    "        \n",
    "        #add duration\n",
    "        duration = int(duration)\n",
    "        if duration > 0:\n",
    "            duration_bucket = duration_to_bucket(duration)\n",
    "            duration_index = content_to_index[\"duration\"]\n",
    "\n",
    "            rows[counter] = track_index\n",
    "            cols[counter] = duration_index\n",
    "            val[counter] = duration_bucket/n_duration_buckets\n",
    "            \n",
    "            counter+=1\n",
    "\n",
    "        #add playcount\n",
    "        if playcount is not None and playcount != \"None\" and not math.isnan(playcount):\n",
    "            playcount = int(playcount)\n",
    "            if playcount > 0: \n",
    "                playcount_bucket = playcount_to_bucket(playcount)\n",
    "                playcount_index = content_to_index[\"playcount\"]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = playcount_index\n",
    "                val[counter] = playcount_bucket/n_playcount_buckets\n",
    "\n",
    "                \n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        if trackno%5000 == 0:\n",
    "            print(\"Track %s of %s. %s s sec.\" %(trackno, tracks_matrix.shape[0], round(time.time()-starttime, 2)))  \n",
    "        trackno += 1\n",
    "\n",
    "    #Implicit ratings: all ratings are 1.             \n",
    "    \n",
    "    rows = rows[:counter]\n",
    "    cols = cols[:counter]\n",
    "    val = val[:counter]\n",
    "    #val = np.ones(rows.shape, dtype = int)\n",
    "\n",
    "    #Build ICM matrix. \n",
    "    ICM_all = sps.coo_matrix((val, (rows, cols)), dtype = int)\n",
    "    \n",
    "    print(\"Built ICM matrix with %s content values.\" %(val.shape[0]))\n",
    "    return ICM_all\n",
    "\n",
    "\n",
    "#Build new ICM\n",
    "ICM_all = build_ICM()\n",
    "print(\"Done!\")\n",
    "\n",
    "#Fun idea! Could we normalize and put all bucket sizes of each continous variables in one variable?\n",
    "\n",
    "#Get old ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50269, 42489)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICM_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ICM!\n"
     ]
    }
   ],
   "source": [
    "#Save the ICM\n",
    "\n",
    "sps.save_npz(\"Saved Matrixes/ICM_all_coo\", ICM_all)\n",
    "print(\"Saved ICM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted\n"
     ]
    }
   ],
   "source": [
    "#Let's convert to csr. \n",
    "ICM_all = ICM_all.tocsr()\n",
    "print(\"Converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1316175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3091270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id\n",
       "0   1316175\n",
       "1   3885714\n",
       "2   3091270\n",
       "3    226759\n",
       "4    230596"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_item_filter(indices):\n",
    "    target_filter = np.zeros((indices), dtype = bool)\n",
    "    for track in target_tracks.values:\n",
    "        track_id = track[0]\n",
    "        track_index = track_to_index[track_id]\n",
    "        target_filter[track_index] = True\n",
    "    return target_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.array([1, 2, 3, 4])\n",
    "f = [True, False, True, True]\n",
    "a[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.999057911974\n",
      "  (0, 1)\t0.821907930312\n",
      "  (0, 2)\t0.88930101191\n",
      "  (1, 0)\t0.0\n",
      "  (1, 1)\t0.0\n",
      "  (1, 2)\t0.875978994544\n",
      "  (2, 0)\t0.68975316884\n",
      "  (2, 1)\t0.507528752437\n",
      "  (2, 2)\t0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipclaesson/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py:274: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using <= is inefficient, try using > instead.\n",
      "  warn(bad_scalar_msg, SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "a = sps.csr_matrix(np.random.rand(3,3))\n",
    "a[0.5 >= a] = 0\n",
    "\n",
    "#print(sps.csr_matrix(a.todense()))\n",
    "print(a)\n",
    "\n",
    "# Vi har en csr.\n",
    "\n",
    "# om vi loopar igenom den och plockar bort noise, sedan skapar ny matrix. \n",
    "\n",
    "\n",
    "#print(sps.csr_matrix(a.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated ids to indexes.\n",
      "Built URM_train with shape 57560,50269\n",
      "Built URM_test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<57560x50269 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 402768 stored elements in Compressed Sparse Row format>,\n",
       " <11766363x5018275 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 403693 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_URM(train_test_split = 0.80):\n",
    "    #Builds urm \n",
    "    \n",
    "    numInteractions = train_final.shape[0]\n",
    "\n",
    "\n",
    "    train_mask = np.random.choice([True,False], numInteractions, [train_test_split, 1-train_test_split])\n",
    "\n",
    "    playlistList = train_final['playlist_id'].values\n",
    "    itemList = train_final['track_id'].values\n",
    "\n",
    "    #Translate ids\n",
    "    playlistList_translated = np.zeros(playlistList.shape)\n",
    "    itemList_translated = np.zeros(itemList.shape)\n",
    "    ratingList = np.ones((playlistList.shape), int)\n",
    "    \n",
    "    for i in range(train_final.shape[0]):\n",
    "        playlistList_translated[i] = playlist_to_index[playlistList[i]]\n",
    "        itemList_translated[i] = track_to_index[itemList[i]]\n",
    "    print(\"Translated ids to indexes.\")\n",
    "    \n",
    "    #Build URM matrix. \n",
    "    URM_train = sps.coo_matrix((ratingList[train_mask], (playlistList_translated[train_mask], itemList_translated[train_mask])))\n",
    "    URM_train = URM_train.tocsr()\n",
    "    print(\"Built URM_train with shape %s,%s\" %(URM_train.shape[0],URM_train.shape[1]))\n",
    "    \n",
    "    \n",
    "    #Build URM_test\n",
    "    test_mask = np.logical_not(train_mask)\n",
    "    URM_test = sps.coo_matrix((ratingList[test_mask], (playlistList[test_mask], itemList[test_mask])))\n",
    "    URM_test = URM_test.tocsr()\n",
    "    print(\"Built URM_test\")\n",
    "    \n",
    "    return URM_train, URM_test\n",
    "\n",
    "build_URM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluation functions\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommendations, at=5):\n",
    "    \n",
    "    starttime = time.time()\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "    \n",
    "    playlists = target_playlists['playlist_id']\n",
    "\n",
    "    for i, playlist_id in enumerate(playlists):\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(\"User %d of %d, %d sec.\" % (i, len(playlists), round(time.time()-starttime)))\n",
    "\n",
    "        relevant_items = URM_test[playlist_id].indices\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommendations.iloc[i,1:6]\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "class Recommender(object):\n",
    "    def __init__(self, URM, target_items, item_ids, k=50, shrinkage=100, similarity='cosine'):\n",
    "        self.dataset = URM\n",
    "        self.target_items = target_items\n",
    "        self.target_item_filter = get_target_item_filter(tracks_final.shape[0])\n",
    "        self.item_ids = item_ids\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        \n",
    "        self.UIM = None\n",
    "        \n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'pearson':\n",
    "            self.distance = Pearson(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'adj-cosine':\n",
    "            self.distance = AdjustedCosine(shrinkage=self.shrinkage)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Recommender(similarity={},k={},shrinkage={})\".format(self.similarity_name, self.k, self.shrinkage)\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        cp = time.time()\n",
    "        #Calculate cosine similarity\n",
    "        ISM = self.distance.compute(X) \n",
    "        print(\"Computed Item-Item similarity matrix. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        \n",
    "        URM = check_matrix(self.dataset, 'csr')\n",
    "        \n",
    "        print(\"Converted URM to csc %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ISM = check_matrix(ISM, 'csr')\n",
    "        print(\"Converted ISM to csc %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        #Filter not targeted tracks. (U x tI). self.target_item_filter has tracks_final.shape[0] as length. \n",
    "        print(\"Nnz: %s\" %(URM.nnz))\n",
    "        \n",
    "        print(URM.shape)\n",
    "        print(self.target_item_filter.shape)\n",
    "        #URM[:,self.target_item_filter.T] = 0\n",
    "        #ISM[self.target_item_filter,self.target_item_filter.T] = 0\n",
    "        \n",
    "        print(\"Filtered not target tracks. %\" %(time.time()-cp))\n",
    "        print(\"Nnz: %s\" %(URM.nnz))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Multiply URM * ISM (U x I * I x I = U x I)\n",
    "        print(URM.shape)\n",
    "        print(ISM.shape)\n",
    "        \n",
    "        #UIM = URM * ISM\n",
    "        #print(\"Computed UIM. %s\" %(time.time()-cp))\n",
    "        #cp = time.time()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        cp = time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #This U x tI UIM can now be used by the recommender function. \n",
    "        \n",
    "        self.W_sparse = UIM\n",
    "\n",
    "    def fit_new(self, X, noise = 0.1):\n",
    "        \n",
    "        ## GET ISM MATRIX (I X I)\n",
    "        cp = time.time()\n",
    "        #Calculate cosine similarity\n",
    "        print(\"Lets compute distance.\")\n",
    "        ISM = self.distance.compute(X) \n",
    "        print(\"Computed Item-Item similarity matrix. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ##GET URM (U X I)\n",
    "        \n",
    "        URM = self.dataset\n",
    "        \n",
    "        ## GET item_ids (1 x I)\n",
    "        \n",
    "        #self.item_ids\n",
    "        \n",
    "        ## FILTER item_ids INTO target_item_ids (1 x tI)\n",
    "        \n",
    "        self.target_item_ids = track_ids[self.target_item_filter]\n",
    "        print(URM.nnz)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## FILTER TARGETED TRACKS\n",
    "        \n",
    "        ISM = ISM[:,self.target_item_filter]\n",
    "        print(\"Filtered targeted tracks in ISM. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        #self.ISM = sps.csr_matrix(self.ISM)\n",
    "        \n",
    "        cp = time.time()  \n",
    "        print(URM.nnz)\n",
    "        #ISM = sps.csr_matrix(ISM)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## CONVERT URM TO CSR\n",
    "        URM = check_matrix(URM, 'csr')\n",
    "        print(\"Checked URM csr %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ##Print dimension\n",
    "        print(URM.shape)\n",
    "        print(ISM.shape)\n",
    "        \n",
    "        ## MULTIPLY URM (U x I) * ISM (I x I)\n",
    "        UIM = URM.dot(ISM)\n",
    "        print(\"Computed URM * ISM %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "        ## MAKE NOT SPARSE\n",
    "        #UIM_dense = UIM.todense()\n",
    "        \n",
    "        ## FILTER UIM into (U x tI) (not needed since I already filtered!)\n",
    "        #UIM_dense = UIM_dense[:,self.target_item_filter]\n",
    "        \n",
    "        ## THIS IS OUR FITTED MODEL\n",
    "        self.UIM = UIM\n",
    "        \n",
    "        return self.UIM\n",
    "\n",
    "        \n",
    "    def recommend_new(self, user_id, at = 5):\n",
    "        ## GET USER_INDEX\n",
    "        user_index = playlist_to_index[user_id]\n",
    "        \n",
    "        # Convert to np.array (why wasn't it before?!)\n",
    "        self.target_item_ids = np.array(self.target_item_ids)\n",
    "        \n",
    "        ## GET ROW CORRESPONDING TO USER (1 x tI)\n",
    "        user_weights = self.UIM[user_index,:].toarray()\n",
    "             \n",
    "        ## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "        top_indexes = np.argsort(user_weights)#[-at:]\n",
    "        top_k_indexes = top_indexes[0, -at:]\n",
    "\n",
    "        ## Translate to indexes\n",
    "        recommendations = self.target_item_ids[top_k_indexes]\n",
    "        \n",
    "        ## RETURN RECOMMENDATIONS\n",
    "        return(recommendations)\n",
    "    \n",
    "    def recommend_dev(self, user_id, at = 5):\n",
    "        print(\"Recommend %s items for user %s!\" %(at, user_id))\n",
    "        ## GET USER_INDEX\n",
    "        user_index = playlist_to_index[user_id]\n",
    "        \n",
    "        # Convert to np.array (why wasn't it before?!)\n",
    "        self.target_item_ids = np.array(self.target_item_ids)\n",
    "        \n",
    "        ## GET ROW CORRESPONDING TO USER (1 x tI)\n",
    "        user_weights = self.UIM[user_index,:].toarray()\n",
    "             \n",
    "        ## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "        top_indexes = np.argsort(user_weights)#[-at:]\n",
    "        print(top_indexes.shape)\n",
    "        top_k_indexes = top_indexes[0, -at:]\n",
    "        print(top_k_indexes.shape)\n",
    "\n",
    "        ## Translate to indexes\n",
    "        recommendations = self.target_item_ids[top_k_indexes]\n",
    "        \n",
    "        ## RETURN RECOMMENDATIONS\n",
    "        return(recommendations)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.dataset[user_id]\n",
    "        print(\"User profile: %s\" %(user_profile))\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "        print(\"Scores: %s\" %(scores))\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(user_id, ranking)\n",
    "        \n",
    "        print(\"Ranking: %s\" %(ranking))\n",
    "        \n",
    "        export = [0,0,0,0,0]\n",
    "        for i in range(5):\n",
    "            t_id = track_to_id[ranking[i]]\n",
    "            export[i] = t_id\n",
    "            \n",
    "        return export\n",
    "    def _filter_seen(self, user_id, ranking):\n",
    "        user_profile = self.dataset[user_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]\n",
    "\n",
    "print(\"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 4]\n",
      "[4 6 7]\n"
     ]
    }
   ],
   "source": [
    "##TESTING\n",
    "\n",
    "\n",
    "user_weights = [1,2,4,3,7,6,1]\n",
    "target_item_ids = np.array([1,2,4,3,7,6,1])\n",
    "at = 3\n",
    "## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "top_indexes = np.argsort(user_weights)[-at:]\n",
    "print(top_indexes)\n",
    "## Translate to indexes\n",
    "recommendations = target_item_ids[top_indexes]\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50269,)\n",
      "(50269,)\n"
     ]
    }
   ],
   "source": [
    "target_item_filter = get_target_item_filter(tracks_final.shape[0])\n",
    "print(target_item_filter.shape)\n",
    "print(track_ids.shape)\n",
    "target_item_ids = track_ids[target_item_filter]\n",
    "\n",
    "#print(target_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ISimilarity(object):\n",
    "    \"\"\"Abstract interface for the similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, shrinkage=10):\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def compute(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cosine(ISimilarity):\n",
    "    def compute(self, X):\n",
    "        # convert to csc matrix for faster column-wise operations\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "        print(\"Converted to csc.\")\n",
    "        # 1) normalize the columns in X\n",
    "        # compute the column-wise norm\n",
    "        # NOTE: this is slightly inefficient. We must copy X to compute the column norms.\n",
    "        # A faster solution is to  normalize the matrix inplace with a Cython function.\n",
    "        Xsq = X.copy()\n",
    "        Xsq.data **= 2\n",
    "        norm = np.sqrt(Xsq.sum(axis=0))\n",
    "        norm = np.asarray(norm).ravel()\n",
    "        norm += 1e-6\n",
    "        # compute the number of non-zeros in each column\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(X.indptr)\n",
    "        # then normalize the values in each column\n",
    "        X.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "\n",
    "        # 2) compute the cosine similarity using the dot-product\n",
    "        dist = X * X.T\n",
    "        print(\"Computed\")\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        dist = dist - sps.dia_matrix((dist.diagonal()[scipy.newaxis, :], [0]), shape=dist.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            dist = self.apply_shrinkage(X, dist)\n",
    "            print(\"Applied shrinkage\")    \n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n",
    "    \n",
    "    def remove_noise(self, X, noise):\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "        i = 0\n",
    "        for row in X:\n",
    "            r = row\n",
    "            row[row > noise] = 1\n",
    "            row[row <= noise] = 0\n",
    "\n",
    "            X[i,:] = r[row]\n",
    "            i += 1\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97774777  0.93313018  0.02228897]]\n",
      "0.933130175107\n",
      "[[ 0.87267959  0.0063126   0.10756306]]\n",
      "0.107563060762\n",
      "[[ 0.30241798  0.35766604  0.74260393]]\n",
      "0.35766603958\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def kkeep_k_largest(X, k):\n",
    "    \n",
    "    M = X.todense()\n",
    "    for row in M: \n",
    "        top_k_idx = np.argsort(row)\n",
    "        print(row)\n",
    "        print(row[0,top_k_idx[0,-k]])\n",
    "        \n",
    "    \n",
    "    \n",
    "a = sps.csr_matrix(np.random.rand(3,3))\n",
    "print(kkeep_k_largest(a,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Can we create a \"better\" URM by matrix factorization? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with train_rate 1\n",
      "Translated ids to indexes.\n",
      "Built URM_train with shape 57560,50268\n",
      "Built URM_test\n",
      "Lets compute distance.\n",
      "Converted to csc.\n",
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n"
     ]
    }
   ],
   "source": [
    "#This is the main script! \n",
    "\n",
    "\n",
    "#1. Fitting the model. \n",
    "\n",
    "#If export is true, the recommendations will be written to file. \n",
    "#If false, evaluation method can be used. \n",
    "export = True\n",
    "\n",
    "if export:\n",
    "    train_rate = 1\n",
    "else:\n",
    "    train_rate = 0.8\n",
    "print(\"Running with train_rate %s\" %(train_rate))\n",
    "\n",
    "URM_train, URM_test = build_URM(train_rate)\n",
    "\n",
    "import time\n",
    "starttime = time.time()\n",
    "rec = Recommender(URM=URM_train, target_items = target_tracks, item_ids = track_ids, shrinkage=10.0)\n",
    "#ICM_idf = ICM_add_IDF(ICM_all)\n",
    "ISM = rec.fit_new(ICM_all) ##Saving outside for quicker restarts. \n",
    "#rec.fit_bad(ICM_all, k = 2000)\n",
    "print(\"Fitted in %s seconds\" %(time.time()-starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10000 playlists, 0.0041310787200927734 sec.\n",
      "1000 out of 10000 playlists, 2.249403953552246 sec.\n",
      "2000 out of 10000 playlists, 4.220192193984985 sec.\n",
      "3000 out of 10000 playlists, 6.1738550662994385 sec.\n",
      "4000 out of 10000 playlists, 8.06681513786316 sec.\n",
      "5000 out of 10000 playlists, 9.956468105316162 sec.\n",
      "6000 out of 10000 playlists, 11.848111152648926 sec.\n",
      "7000 out of 10000 playlists, 13.741897106170654 sec.\n",
      "8000 out of 10000 playlists, 15.575993299484253 sec.\n",
      "9000 out of 10000 playlists, 17.38797116279602 sec.\n",
      "Saved to file: \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#2. Creating recommendations. \n",
    "zeros = np.zeros((target_playlists.size, 6), dtype = int)\n",
    "recommendations = pd.DataFrame(zeros)\n",
    "recommendations.columns = ['playlist_id', 1, 2, 3, 4, 5]\n",
    "\n",
    "counter = 0\n",
    "starttime = time.time()\n",
    "for playlist_id in target_playlists['playlist_id']:\n",
    "\n",
    "    if counter % 1000 == 0: \n",
    "        print (\"%s out of 10000 playlists, %s sec.\" %(counter, time.time()-starttime))\n",
    "\n",
    "    playlist_id_translated = playlist_to_index[int(playlist_id)]\n",
    "    recommendations.iloc[counter, 1:6] = rec.recommend_new(playlist_id, 5)\n",
    "    recommendations.iloc[counter, 0] = playlist_id\n",
    "    counter += 1\n",
    "\n",
    "if export:\n",
    "    filename = \"recommendations_6/11_\"\n",
    "    np.savetxt(\"output/recommendations_8nov.csv\",recommendations, fmt = '%s,%s %s %s %s %s', header = \"playlist_id,track_ids\", newline = \"\\n\")\n",
    "    print(\"Saved to file: \")\n",
    "\n",
    "#print(recommendations)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Want to evaluate? \n",
    "evaluate_algorithm(URM_test, recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TESTING THE REC FUNCTION - SHOULD WORK\n",
    "#Fitted in 172.8 seconds\n",
    "\n",
    "rec_dev = Recommender(URM=URM_train, target_items = target_tracks, item_ids = track_ids, shrinkage=0.0)\n",
    "rec_dev.UIM = rec.UIM\n",
    "rec_dev.target_item_ids = rec.target_item_ids\n",
    "\n",
    "zeros = np.zeros((1, 6), dtype = int)\n",
    "recommendations = pd.DataFrame(zeros)\n",
    "recommendations.columns = ['playlist_id', 1, 2, 3, 4, 5]\n",
    "recommendations.iloc[counter, 1:6] = rec_dev.recommend_dev(playlist_to_id[30680], 5)\n",
    "recommendations.iloc[counter, 0] = playlist_to_id[30680]\n",
    "\n",
    "print(recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "rec = Recommender(URM=URM_train, shrinkage=0.0)\n",
    "rec.fit(ICM_all)\n",
    "print(\"Done in %s seconds\" %(time.time()-starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ICM_add_IDF(ICM): \n",
    "    num_tot_items = ICM_all.shape[0]\n",
    "\n",
    "    # let's count how many items have a certain feature\n",
    "    items_per_feature = (ICM_all > 0).sum(axis=0)\n",
    "\n",
    "    IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "\n",
    "    print(ICM_all.shape)\n",
    "    print(IDF.shape)\n",
    "    ICM_idf = sps.csr_matrix(ICM_all, dtype=np.float64)\n",
    "    # compute the number of non-zeros in each col\n",
    "    # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "    col_nnz = np.diff(check_matrix(ICM_idf, 'csc').indptr)\n",
    "    print(col_nnz.shape)\n",
    "    print(ICM_idf.shape)\n",
    "    print(IDF.shape)\n",
    "    # then normalize the values in each col\n",
    "    ICM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "    return ICM_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_idf = BasicItemKNNRecommender(URM=URM_train, shrinkage=0.0, k=50)\n",
    "rec_idf.fit(ICM_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_algorithm(URM_test, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_file():\n",
    "    #Saves the recommendations dataframe to the .csv-file. \n",
    "    np.savetxt(\"output/recommendations_more_content.csv\",recommendations, fmt = '%s,%s %s %s %s %s', header = \"playlist_id,track_ids\", newline = \"\\n\")\n",
    "    \n",
    "    \n",
    "def test():\n",
    "    #Do something\n",
    "    print(\"Result: \")\n",
    "    pass\n",
    "\n",
    "\n",
    "save_to_file()\n",
    "print(recommendations.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
