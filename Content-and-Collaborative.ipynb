{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sps\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "#train_final.csv - the training set of interactions\n",
    "train_final = pd.read_csv('input/train_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#tracks_final.csv - supplementary information about the items\n",
    "tracks_final = pd.read_csv('input/tracks_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#playlists_final.csv - supplementary information about the users\n",
    "playlists_final = pd.read_csv('input/playlists_final.csv', delimiter = \"\\t\");\n",
    "\n",
    "#target_playlists.csv - the set of target playlists that will receive recommendations\n",
    "target_playlists = pd.read_csv('input/target_playlists.csv');\n",
    "\n",
    "#target_tracks.csv - the set of target items (tracks) to be recommended\n",
    "target_tracks = pd.read_csv('input/target_tracks.csv');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Algorithm still not as good as it should be. \n",
    "#\n",
    "#Bug hunting: \n",
    "#    - Bad content tags? Yes, alNone was a tag. Removed but still not improved. \n",
    "#    - Bad URM. The URM was only trained with 50% of data. Now fixed, improved a lot. \n",
    "#    - \n",
    "#    - \n",
    "#    - \n",
    "#    - \n",
    "#    - \n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_final now contains 945579 interactions. \n",
      "Tracks_final now contains 74542 tracks. \n"
     ]
    }
   ],
   "source": [
    "#This step is not needed yet, will make ratings worse! \n",
    "\n",
    "def get_relevant_tracks():\n",
    "    #Now we want to remove some redundant stuff. \n",
    "\n",
    "    #We will remove all songs which are not occurring more than 10 times in train_final\n",
    "    #Nevertheless, we still want to keep all tracks which are in the target tracks.  \n",
    "\n",
    "    popularity = train_final.groupby(by=\"track_id\").playlist_id.nunique().to_frame()\n",
    "\n",
    "    #remove index name\n",
    "    popularity.reset_index(level = 0, inplace = True)\n",
    "\n",
    "    #Rename the columns\n",
    "    popularity.columns = ['track_id','occurrences']\n",
    "\n",
    "    #Remove all targeted tracks - TESTED, working as expected\n",
    "    tracks_relevant = popularity[~popularity['track_id'].isin(target_tracks['track_id'])]\n",
    "\n",
    "    #Remove tracks occurring less than 10 times\n",
    "    tracks_relevant = tracks_relevant[tracks_relevant['occurrences'] > 4]\n",
    "\n",
    "    #Add the targeteted tracks back again\n",
    "    tracks_relevant = pd.concat([tracks_relevant, target_tracks])\n",
    "\n",
    "    return(tracks_relevant)\n",
    "\n",
    "    print(\"Removed %s redundant tracks which occured less than 10 times.\" %(tracks_final-tracks_relevant))\n",
    "\n",
    "tracks_relevant = get_relevant_tracks()\n",
    "\n",
    "#Remove irrelevant tracks from train_final and tracks_final\n",
    "train_final = train_final[train_final['track_id'].isin(tracks_relevant['track_id'])]\n",
    "\n",
    "print(\"Train_final now contains %s interactions. \" %(train_final.shape[0]))\n",
    "\n",
    "tracks_final = tracks_final[tracks_final['track_id'].isin(tracks_relevant['track_id'])]\n",
    "\n",
    "print(\"Tracks_final now contains %s tracks. \"%(tracks_final.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>playcount</th>\n",
       "      <th>album</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2972914</td>\n",
       "      <td>144</td>\n",
       "      <td>224000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[54087, 1757, 1718, 116712, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750239</td>\n",
       "      <td>246</td>\n",
       "      <td>157000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[189631, 3424, 177424, 46208, 205245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550729</td>\n",
       "      <td>144</td>\n",
       "      <td>217000</td>\n",
       "      <td>554.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 46869, 183258, 54337]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2169950</td>\n",
       "      <td>144</td>\n",
       "      <td>207000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 70618, 207003, 109806, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2256817</td>\n",
       "      <td>144</td>\n",
       "      <td>218000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 189631, 49166, 116712]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id  artist_id  duration  playcount album  \\\n",
       "0   2972914        144    224000       49.0   [7]   \n",
       "1   2750239        246    157000        1.0   [8]   \n",
       "2   1550729        144    217000      554.0   [9]   \n",
       "3   2169950        144    207000      200.0   [9]   \n",
       "5   2256817        144    218000        2.0   [9]   \n",
       "\n",
       "                                     tags  \n",
       "0     [54087, 1757, 1718, 116712, 189631]  \n",
       "1   [189631, 3424, 177424, 46208, 205245]  \n",
       "2   [54087, 109806, 46869, 183258, 54337]  \n",
       "3  [54087, 70618, 207003, 109806, 116712]  \n",
       "5  [54087, 109806, 189631, 49166, 116712]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets take a look at the tags.\n",
    "tracks_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Translating all content ids into indexes.\n",
    "\n",
    "#We need to create buckets for the playcount and duration. \n",
    "#Lets create buckets and a help function for the duration. \n",
    "\n",
    "n_duration_buckets = 3\n",
    "def duration_to_bucket(duration, alternative = 2):\n",
    "    if (alternative == 1):\n",
    "        n_duration_buckets = 8\n",
    "        if duration <= 0:\n",
    "            print(\"Null duration reached bucket function. \")\n",
    "            return None\n",
    "        elif duration < 90000: #not a song\n",
    "            return 1\n",
    "        elif duration < 140000: #short song\n",
    "            return 2\n",
    "        elif duration < 220000: #radio song\n",
    "            return 3\n",
    "        elif duration < 340000: #normal song\n",
    "            return 4\n",
    "        elif duration < 480000: #long song\n",
    "            return 5\n",
    "        elif duration < 720000: #really long\n",
    "            return 6\n",
    "        elif duration < 1200000: #super long\n",
    "            return 7\n",
    "        elif duration >= 1200000: #mixtape/compilation\n",
    "            return 8\n",
    "    elif(alternative == 2):\n",
    "        n_duration_buckets = 3\n",
    "        if duration <= 0:\n",
    "            print(\"Null duration reached bucket function. \")\n",
    "            return None\n",
    "        elif duration <= 150000: #very short\n",
    "            return 1\n",
    "        elif duration > 150000 and duration < 720000: #very long\n",
    "            return 2\n",
    "        elif duration >= 720000: #mixtape/compilation\n",
    "            return 3\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "\n",
    "n_playcount_buckets = 7\n",
    "def playcount_to_bucket(playcount):\n",
    "    if playcount <= 0 or playcount is None:\n",
    "        print(\"Null playcount reached bucket function. \")\n",
    "        return None\n",
    "    elif playcount < 254: #0,4 percentile not popular\n",
    "        return 1\n",
    "    elif playcount < 881: #0,6 perc: known\n",
    "        return 2\n",
    "    elif playcount < 1560: #0,7 popular\n",
    "        return 3\n",
    "    elif playcount < 2808: #0,8 very popular\n",
    "        return 4\n",
    "    elif playcount < 5900: #0,9 hits\n",
    "        return 5\n",
    "    elif playcount < 10494: #0,95 super hits\n",
    "        return 6\n",
    "    elif playcount >= 10494: # mega hits\n",
    "        return 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58358\n",
      "21661 albums. 27607 expected.\n",
      "13915 artists. 17537 expected.\n"
     ]
    }
   ],
   "source": [
    "tracks_final['tags'].head()\n",
    "\n",
    "content_to_index = {}\n",
    "content_to_id = {}\n",
    "content_counter = 0\n",
    "\n",
    "#Lets translate the tags to indexes.\n",
    "for row in tracks_final['tags']:\n",
    "    tags = row.strip('[ ]').split(', ')\n",
    "    for tag in tags:\n",
    "        if len(tag) > 0: \n",
    "            tag = \"t\"+tag\n",
    "            if not(tag in content_to_index):\n",
    "                content_to_index[tag] = content_counter\n",
    "                content_to_id[content_counter] = tag\n",
    "                content_counter += 1;\n",
    "                \n",
    "#Lets translate album into indexes\n",
    "albumcount = 0 # 27607\n",
    "for album in tracks_final['album']:\n",
    "    album = album.strip('[ ]')\n",
    "    if album != None and album != \"None\" and len(album) > 0: #None should not be considered content\n",
    "        album = \"al\"+album\n",
    "        if album == \"alNone\":\n",
    "            print(album)\n",
    "        if not(album in content_to_index):\n",
    "            content_to_index[album] = content_counter\n",
    "            content_to_id[content_counter] = album\n",
    "            content_counter += 1\n",
    "            albumcount += 1\n",
    "\n",
    "#Lets translate artist_id into indexes \n",
    "artistcount = 0 #17537\n",
    "for artist in tracks_final['artist_id']:\n",
    "    artist = str(artist)\n",
    "    if artist != None and artist != \"None\" and len(artist) > 0: #None should not be considered content\n",
    "        artist = \"ar\"+artist\n",
    "        if not(artist in content_to_index):\n",
    "            content_to_index[artist] = content_counter\n",
    "            content_to_id[content_counter] = artist\n",
    "            content_counter += 1\n",
    "            artistcount += 1\n",
    "        \n",
    "\"\"\"\n",
    "#Lets translate the duration buckets into indexes. \n",
    "for bucket in range(n_duration_buckets): \n",
    "    bucket = \"d\"+str(bucket+1)\n",
    "    content_to_index[bucket] = content_counter\n",
    "    content_to_id[content_counter] = bucket\n",
    "    print(\"added %s\" %(bucket))\n",
    "    content_counter += 1\n",
    "\n",
    "#Lets translate the playcount buckets into indexes. \n",
    "for playcount in range(n_playcount_buckets): \n",
    "    playcount = \"p\"+str(playcount+1)\n",
    "    content_to_index[playcount] = content_counter\n",
    "    content_to_id[content_counter] = playcount\n",
    "    \n",
    "    content_counter += 1\n",
    "\n",
    "\n",
    "## Alternative 2: Just one content type per continous variable. \n",
    "#Fun thing to try: can I add all duration/playcounts in one col, normalizing from 0-1? \n",
    "\n",
    "\n",
    "content_to_index[\"duration\"] = content_counter\n",
    "content_to_id[content_counter] = \"duration\"\n",
    "content_counter += 1\n",
    "\n",
    "content_to_index[\"playcount\"] = content_counter\n",
    "content_to_id[content_counter] = \"playcount\"\n",
    "content_counter += 1\n",
    "\"\"\"\n",
    "\n",
    "print(len(content_to_index))\n",
    "print(\"%s albums. 27607 expected.\" %albumcount)\n",
    "print(\"%s artists. 17537 expected.\" %artistcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 57561 playlists with 74542 unique tracks with 58358 unique content types. \n"
     ]
    }
   ],
   "source": [
    "#If we translate each track_id to a track_index which will serve as matrix index, we can save a lot of time. \n",
    "\n",
    "\n",
    "#We need a way to get from track_id to index in O(1).\n",
    "#Let's create a dictionary\n",
    "\n",
    "track_to_id = {}\n",
    "track_to_index = {}\n",
    "track_ids = tracks_final['track_id']\n",
    "\n",
    "counter = 0;\n",
    "for track_id in tracks_final['track_id']:\n",
    "    track_id = int(track_id)\n",
    "    track_to_index[track_id] = counter\n",
    "    track_to_id[counter] = track_id\n",
    "    counter += 1;\n",
    "    \n",
    "#and a way to get from playlist_id to index in O(1)\n",
    "\n",
    "\n",
    "playlist_to_index = {}\n",
    "playlist_to_id = {}\n",
    "counter = 0; \n",
    "for playlist_id in playlists_final['playlist_id']:\n",
    "    playlist_id = int(playlist_id)\n",
    "    playlist_to_index[playlist_id] = counter\n",
    "    playlist_to_id[counter] = playlist_id\n",
    "    counter += 1;\n",
    "    \n",
    "print(\"We have {} playlists with {} unique tracks with {} unique content types. \".format(len(playlist_to_index), len(track_to_index), len(content_to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can create an Item Content Matrix. \n",
    "\n",
    "#ICM_all = np.zeros((len(tracks_indexes), len(tags_indexes)), int)\n",
    "#ICM_all = sps.coo_matrix((len(track_to_index), len(content_to_index)), int)\n",
    "#print(ICM_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>playcount</th>\n",
       "      <th>album</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2972914</td>\n",
       "      <td>144</td>\n",
       "      <td>224000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[54087, 1757, 1718, 116712, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750239</td>\n",
       "      <td>246</td>\n",
       "      <td>157000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[189631, 3424, 177424, 46208, 205245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550729</td>\n",
       "      <td>144</td>\n",
       "      <td>217000</td>\n",
       "      <td>554.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 46869, 183258, 54337]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2169950</td>\n",
       "      <td>144</td>\n",
       "      <td>207000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 70618, 207003, 109806, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2256817</td>\n",
       "      <td>144</td>\n",
       "      <td>218000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[54087, 109806, 189631, 49166, 116712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>474864</td>\n",
       "      <td>928</td>\n",
       "      <td>193000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[205245, 81223, 11056, 267, 3982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1523190</td>\n",
       "      <td>928</td>\n",
       "      <td>206000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[205245, 11056, 81223, 4425, 189631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1373492</td>\n",
       "      <td>928</td>\n",
       "      <td>237000</td>\n",
       "      <td>2896.0</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[81223, 11056, 205245, 189631, 3982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3387498</td>\n",
       "      <td>928</td>\n",
       "      <td>245000</td>\n",
       "      <td>9622.0</td>\n",
       "      <td>[31]</td>\n",
       "      <td>[81223, 189631, 205245, 4425, 50764]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>699524</td>\n",
       "      <td>928</td>\n",
       "      <td>294000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>[26]</td>\n",
       "      <td>[11056, 205245, 81223, 4425, 189631]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_id  artist_id  duration  playcount album  \\\n",
       "0    2972914        144    224000       49.0   [7]   \n",
       "1    2750239        246    157000        1.0   [8]   \n",
       "2    1550729        144    217000      554.0   [9]   \n",
       "3    2169950        144    207000      200.0   [9]   \n",
       "5    2256817        144    218000        2.0   [9]   \n",
       "7     474864        928    193000       73.0  [22]   \n",
       "9    1523190        928    206000       10.0  [22]   \n",
       "10   1373492        928    237000     2896.0  [22]   \n",
       "11   3387498        928    245000     9622.0  [31]   \n",
       "12    699524        928    294000       22.0  [26]   \n",
       "\n",
       "                                      tags  \n",
       "0      [54087, 1757, 1718, 116712, 189631]  \n",
       "1    [189631, 3424, 177424, 46208, 205245]  \n",
       "2    [54087, 109806, 46869, 183258, 54337]  \n",
       "3   [54087, 70618, 207003, 109806, 116712]  \n",
       "5   [54087, 109806, 189631, 49166, 116712]  \n",
       "7        [205245, 81223, 11056, 267, 3982]  \n",
       "9     [205245, 11056, 81223, 4425, 189631]  \n",
       "10    [81223, 11056, 205245, 189631, 3982]  \n",
       "11    [81223, 189631, 205245, 4425, 50764]  \n",
       "12    [11056, 205245, 81223, 4425, 189631]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 0 of 74542. 0.0 s sec.\n",
      "Track 5000 of 74542. 0.12 s sec.\n",
      "Track 10000 of 74542. 0.24 s sec.\n",
      "Track 15000 of 74542. 0.37 s sec.\n",
      "Track 20000 of 74542. 0.49 s sec.\n",
      "Track 25000 of 74542. 0.61 s sec.\n",
      "Track 30000 of 74542. 0.73 s sec.\n",
      "Track 35000 of 74542. 0.85 s sec.\n",
      "Track 40000 of 74542. 0.96 s sec.\n",
      "Track 45000 of 74542. 1.08 s sec.\n",
      "Track 50000 of 74542. 1.2 s sec.\n",
      "Track 55000 of 74542. 1.32 s sec.\n",
      "Track 60000 of 74542. 1.43 s sec.\n",
      "Track 65000 of 74542. 1.56 s sec.\n",
      "Track 70000 of 74542. 1.69 s sec.\n",
      "Built ICM matrix with 492756 content values.\n",
      "21661 albums. 27607 expected.\n",
      "13915 artists. 17537 expected.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#So let's fill the ICM with our data.\n",
    "import math\n",
    "\n",
    "def build_ICM():\n",
    "    \n",
    "    no_interactions = train_final.shape[0]\n",
    "    \n",
    "    tracks_matrix = tracks_final.as_matrix()\n",
    "    rows = np.zeros((no_interactions,), dtype = int)\n",
    "    cols = np.zeros((no_interactions,), dtype = int)\n",
    "    val = np.zeros((no_interactions,), dtype = int)\n",
    "    #val[i] = value of row[i] col[i]\n",
    "    #val = []\n",
    "    counter = 0\n",
    "    starttime = time.time()\n",
    "    lasttime = starttime\n",
    "    trackno = 0\n",
    "    addedalbums = {} #for testing\n",
    "    addedartists = {} # for testing\n",
    "    for track in tracks_matrix: \n",
    "        track_id, artist_id, duration, playcount, album, tags = np.split(track, 6)\n",
    "\n",
    "        #Get track index\n",
    "        track_index = track_to_index[int(track_id[0])]\n",
    "\n",
    "        \n",
    "        #add artist\n",
    "        \n",
    "        artist_index = content_to_index[\"ar\"+str(artist_id[0])]\n",
    "        addedartists[artist_index] = 1\n",
    "        \n",
    "        rows[counter] = track_index\n",
    "        cols[counter] = artist_index\n",
    "        val[counter] = 1\n",
    "        counter += 1\n",
    "\n",
    "        #add album\n",
    "        album = album[0].strip(\"[ ]\")\n",
    "\n",
    "        if album != None and len(album) > 0 and not album == \"None\":\n",
    "            album_index = content_to_index[\"al\"+album]\n",
    "            addedalbums[album_index] = 1 #testing\n",
    "            \n",
    "            rows[counter] = track_index\n",
    "            cols[counter] = album_index\n",
    "            val[counter] = 1\n",
    "            counter += 1\n",
    "\n",
    "        #add tags\n",
    "        tags = tags[0].strip('[ ]').split(', ')\n",
    "\n",
    "        for tag in tags: \n",
    "            if len(tag) > 0:\n",
    "                tag = \"t\"+tag\n",
    "                tag_index = content_to_index[tag]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = tag_index\n",
    "                val[counter] = 1\n",
    "                \n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        ## ALT 1: Continuous variables in different content types. \n",
    "        \n",
    "        #add duration\n",
    "        duration = int(duration)\n",
    "        if duration > 0:\n",
    "            duration_bucket = duration_to_bucket(duration)\n",
    "            if duration_bucket > 0:   \n",
    "                duration_index = content_to_index[\"d\"+str(duration_bucket)]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = duration_index\n",
    "\n",
    "                counter+=1\n",
    "        \n",
    "        #add playcount\n",
    "        if playcount is not None and playcount != \"None\" and not math.isnan(playcount):\n",
    "            playcount = int(playcount)\n",
    "            if playcount > 0: \n",
    "                playcount_bucket = playcount_to_bucket(playcount)\n",
    "                playcount_index = content_to_index[\"p\"+str(playcount_bucket)]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = playcount_index\n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ## ALT 2: Continuous variables in one content type. \n",
    "        \n",
    "        #add duration\n",
    "        duration = int(duration)\n",
    "        if duration > 0:\n",
    "            duration_bucket = duration_to_bucket(duration)\n",
    "            duration_index = content_to_index[\"duration\"]\n",
    "\n",
    "            rows[counter] = track_index\n",
    "            cols[counter] = duration_index\n",
    "            val[counter] = duration_bucket/n_duration_buckets\n",
    "            \n",
    "            counter+=1\n",
    "\n",
    "        #add playcount\n",
    "        if playcount is not None and playcount != \"None\" and not math.isnan(playcount):\n",
    "            playcount = int(playcount)\n",
    "            if playcount > 0: \n",
    "                playcount_bucket = playcount_to_bucket(playcount)\n",
    "                playcount_index = content_to_index[\"playcount\"]\n",
    "\n",
    "                rows[counter] = track_index\n",
    "                cols[counter] = playcount_index\n",
    "                val[counter] = playcount_bucket/n_playcount_buckets\n",
    "\n",
    "                \n",
    "                counter+=1\n",
    "        \"\"\"\n",
    "        if trackno%5000 == 0:\n",
    "            print(\"Track %s of %s. %s s sec.\" %(trackno, tracks_matrix.shape[0], round(time.time()-starttime, 2)))  \n",
    "        trackno += 1\n",
    "\n",
    "    #Implicit ratings: all ratings are 1.             \n",
    "    \n",
    "    rows = rows[:counter]\n",
    "    cols = cols[:counter]\n",
    "    val = val[:counter]\n",
    "    #val = np.ones(rows.shape, dtype = int)\n",
    "\n",
    "    #Build ICM matrix. \n",
    "    ICM_all = sps.coo_matrix((val, (rows, cols)), dtype = int)\n",
    "    \n",
    "    print(\"Built ICM matrix with %s content values.\" %(val.shape[0]))\n",
    "    \n",
    "    print(\"%s albums. 27607 expected.\" %len(addedalbums))\n",
    "    print(\"%s artists. 17537 expected.\" %len(addedartists))\n",
    "    \n",
    "    return ICM_all\n",
    "\n",
    "\n",
    "#Build new ICM\n",
    "ICM_all = build_ICM()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "#Get old ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'alNone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7d21ccf71c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alNone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'alNone'"
     ]
    }
   ],
   "source": [
    "ICM_all.shape\n",
    "\"\"\"\n",
    "with open(\"output/content_tags.txt\",'w') as f:\n",
    "    for content in content_to_index: \n",
    "        f.write(content+\"\\n\")\n",
    "\"\"\"\n",
    "print(content_to_index[\"alNone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the ICM\n",
    "\n",
    "sps.save_npz(\"Saved Matrixes/ICM_all_coo\", ICM_all)\n",
    "print(\"Saved ICM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's convert to csr. \n",
    "ICM_all = ICM_all.tocsr()\n",
    "print(\"Converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_item_filter(indices):\n",
    "    target_filter = np.zeros((indices), dtype = bool)\n",
    "    for track in target_tracks.values:\n",
    "        track_id = track[0]\n",
    "        track_index = track_to_index[track_id]\n",
    "        target_filter[track_index] = True\n",
    "    print(\"Created filter preserving %s out of %s \" %(np.count_nonzero(target_filter),target_filter.shape[0]))\n",
    "    return target_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([1, 2, 3, 4])\n",
    "f = [True, False, True, True]\n",
    "a[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sps.csr_matrix(np.random.rand(3,3))\n",
    "a[0.5 >= a] = 0\n",
    "\n",
    "#print(sps.csr_matrix(a.todense()))\n",
    "print(a)\n",
    "\n",
    "# Vi har en csr.\n",
    "\n",
    "# om vi loopar igenom den och plockar bort noise, sedan skapar ny matrix. \n",
    "\n",
    "\n",
    "#print(sps.csr_matrix(a.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 756234. False: 189345. Tot: 945579\n",
      "Built URM_test\n",
      "Total datapoints: 945579. Expected: 945579\n",
      "(57560, 74542)\n",
      "(57560, 74542)\n"
     ]
    }
   ],
   "source": [
    "def build_URM(train_test_split = 0.80):\n",
    "    #Builds urm \n",
    "    \n",
    "    #train_test_split = 1\n",
    "    \n",
    "    numInteractions = train_final.shape[0]\n",
    "\n",
    "    train_mask = np.random.choice(a = [True,False], size = numInteractions, p = [train_test_split, 1-train_test_split])\n",
    "    \n",
    "    playlistList = train_final['playlist_id'].values\n",
    "    itemList = train_final['track_id'].values\n",
    "\n",
    "    #Translate ids\n",
    "    playlistList_translated = np.zeros(playlistList.shape)\n",
    "    itemList_translated = np.zeros(itemList.shape)\n",
    "    ratingList = np.ones((playlistList.shape), int)\n",
    "    \n",
    "    tru = train_mask[train_mask == True].shape[0]\n",
    "    fal = (train_mask[train_mask == False].shape[0])\n",
    "    \n",
    "    print(\"True: %s. False: %s. Tot: %s\" %(tru, fal, (tru+fal)))\n",
    "\n",
    "    \n",
    "    for i in range(train_final.shape[0]):\n",
    "        playlistList_translated[i] = playlist_to_index[playlistList[i]]\n",
    "        itemList_translated[i] = track_to_index[itemList[i]]\n",
    "    #print(\"Translated ids to indexes.\")\n",
    "    \n",
    "    #Build URM matrix. \n",
    "    URM_train = sps.coo_matrix((ratingList[train_mask], (playlistList_translated[train_mask], itemList_translated[train_mask])))\n",
    "    URM_train = URM_train.tocsr()\n",
    "    #print(\"Built URM_train with shape %s,%s\" %(URM_train.shape[0],URM_train.shape[1]))\n",
    "    \n",
    "    if train_test_split < 1: \n",
    "        #Build URM_test\n",
    "        test_mask = np.logical_not(train_mask)\n",
    "        URM_test = sps.coo_matrix((ratingList[test_mask], (playlistList_translated[test_mask], itemList_translated[test_mask])))\n",
    "        URM_test = URM_test.tocsr()\n",
    "        print(\"Built URM_test\")\n",
    "        testsize = (test_mask[test_mask == True].shape[0])\n",
    "\n",
    "    else: \n",
    "        URM_test = sps.csc_matrix((10, 10), dtype=np.int8)\n",
    "        testsize = 0\n",
    "    \n",
    "    \n",
    "    trainsize = train_mask[train_mask == True].shape[0]\n",
    "    totsize = trainsize + testsize\n",
    "    print(\"Total datapoints: %s. Expected: %s\" %(totsize,numInteractions))\n",
    "\n",
    "    \n",
    "    print(URM_train.shape)\n",
    "    print(URM_test.shape)\n",
    "    \n",
    "    return URM_train, URM_test\n",
    "\n",
    "URM_train, URM_test = build_URM(0.8)\n",
    "\n",
    "#Problem: The number of true/false values is not consistent.. Gives problems when testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URM_train contains 756234 interactions. Expected 1040422\n",
      "Train: 0.7991. Test: 0.2009\n"
     ]
    }
   ],
   "source": [
    "#Testing the URM builder.\n",
    "print(\"URM_train contains %s interactions. Expected 1040422\" %URM_train.nnz)\n",
    "testcount = 0\n",
    "traincount = 0\n",
    "itr = 10000\n",
    "for playlist_id, track_id in train_final[0:itr].values: \n",
    "    if (URM_train[playlist_to_index[playlist_id],track_to_index[track_id]]) > 0: \n",
    "        #print(\"Playlist %s with index %s and track %s with index %s was not in URM_train.\" %(playlist_id, playlist_to_index[playlist_id],track_id, track_to_index[track_id]))\n",
    "        traincount += 1\n",
    "    elif (URM_test[playlist_to_index[playlist_id],track_to_index[track_id]]) > 0:\n",
    "        testcount += 1\n",
    "        \n",
    "print(\"Train: %s. Test: %s\"%(traincount/itr, testcount/itr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluation functions\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommendations, at=5):\n",
    "    \n",
    "    starttime = time.time()\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "    \n",
    "    playlists = target_playlists['playlist_id']\n",
    "\n",
    "    for i, playlist_id in enumerate(playlists):\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(\"User %d of %d, %d sec.\" % (i, len(playlists), round(time.time()-starttime)))\n",
    "\n",
    "        relevant_items = URM_test[playlist_to_index[playlist_id]].indices\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommendations.iloc[i,1:6]\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "class Recommender(object):\n",
    "    def __init__(self, URM, target_items, item_ids, k=50, shrinkage=100, similarity='cosine', filter_method = 'content', topK = 100):\n",
    "        self.dataset = URM\n",
    "        self.target_items = target_items\n",
    "        self.target_item_filter = get_target_item_filter(tracks_final.shape[0])\n",
    "        self.item_ids = item_ids\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        self.filter_method = filter_method\n",
    "        self.topK = topK\n",
    "        \n",
    "        self.UIM = None\n",
    "        \n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'pearson':\n",
    "            self.distance = Pearson(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'adj-cosine':\n",
    "            self.distance = AdjustedCosine(shrinkage=self.shrinkage)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Recommender(similarity={},k={},shrinkage={})\".format(self.similarity_name, self.k, self.shrinkage)\n",
    "\n",
    "    \n",
    "    def fit_new(self, X, noise = 0.1):\n",
    "        \n",
    "        ## GET ISM MATRIX (I X I)\n",
    "        cp = time.time()\n",
    "        #Calculate cosine similarity\n",
    "        print(\"Using %s filtering with TopK = %s to compute distance.\" %(self.filter_method, self.topK))\n",
    "        if (self.filter_method == 'collaborative'):\n",
    "            cosine_cython = Cosine_Similarity(URM_train, TopK=self.topK)\n",
    "            start_time = time.time()\n",
    "            ISM = cosine_cython.compute_similarity()\n",
    "        else:\n",
    "            ISM = self.distance.compute(X)\n",
    "        print(\"Computed Item-Item similarity matrix. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ##GET URM (U X I)\n",
    "        \n",
    "        URM = self.dataset\n",
    "        \n",
    "        ## GET item_ids (1 x I)\n",
    "        \n",
    "        #self.item_ids\n",
    "        \n",
    "        ## FILTER item_ids INTO target_item_ids (1 x tI)\n",
    "        \n",
    "        self.target_item_ids = track_ids[self.target_item_filter]\n",
    "        print(URM.nnz)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## FILTER TARGETED TRACKS\n",
    "        #Maybe this is not working as expected - are we filtering the right tracks? \n",
    "        \n",
    "        ISM = ISM[:,self.target_item_filter]\n",
    "        print(\"Filtered targeted tracks in ISM. %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        #self.ISM = sps.csr_matrix(self.ISM)\n",
    "        \n",
    "        cp = time.time()  \n",
    "        print(URM.nnz)\n",
    "        #ISM = sps.csr_matrix(ISM)\n",
    "        print(ISM.nnz)\n",
    "        \n",
    "        ## CONVERT URM TO CSR\n",
    "        URM = check_matrix(URM, 'csr')\n",
    "        print(\"Checked URM csr %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "        ##Print dimension\n",
    "        print(URM.shape)\n",
    "        print(ISM.shape)\n",
    "        \n",
    "        ## MULTIPLY URM (U x I) * ISM (I x I)\n",
    "        UIM = URM.dot(ISM)\n",
    "        print(\"Computed URM * ISM %s \" %(time.time()-cp))\n",
    "        cp = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "        ## MAKE NOT SPARSE\n",
    "        #UIM_dense = UIM.todense()\n",
    "        \n",
    "        ## FILTER UIM into (U x tI) (not needed since I already filtered!)\n",
    "        #UIM_dense = UIM_dense[:,self.target_item_filter]\n",
    "        \n",
    "        ## THIS IS OUR FITTED MODEL\n",
    "        self.UIM = UIM\n",
    "        \n",
    "        return self.UIM\n",
    "\n",
    "        \n",
    "    def recommend_new(self, user_id, at = 5):\n",
    "        ## GET USER_INDEX\n",
    "        user_index = playlist_to_index[user_id]\n",
    "        \n",
    "        # Convert to np.array (why wasn't it before?!)\n",
    "        self.target_item_ids = np.array(self.target_item_ids)\n",
    "        \n",
    "        ## GET ROW CORRESPONDING TO USER (1 x tI)\n",
    "        user_weights = self.UIM[user_index,:].toarray()\n",
    "             \n",
    "        ## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "        top_indexes = np.argsort(user_weights)#[-at:]\n",
    "        top_k_indexes = top_indexes[0, -at:]\n",
    "\n",
    "        ## Translate to indexes\n",
    "        recommendations = self.target_item_ids[top_k_indexes]\n",
    "        \n",
    "        ## RETURN RECOMMENDATIONS\n",
    "        return(recommendations)\n",
    "    \n",
    "    def recommend_dev(self, user_id, at = 5):\n",
    "        print(\"Recommend %s items for user %s!\" %(at, user_id))\n",
    "        ## GET USER_INDEX\n",
    "        user_index = playlist_to_index[user_id]\n",
    "        \n",
    "        # Convert to np.array (why wasn't it before?!)\n",
    "        self.target_item_ids = np.array(self.target_item_ids)\n",
    "        \n",
    "        ## GET ROW CORRESPONDING TO USER (1 x tI)\n",
    "        user_weights = self.UIM[user_index,:].toarray()\n",
    "             \n",
    "        ## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "        top_indexes = np.argsort(user_weights)#[-at:]\n",
    "        print(top_indexes.shape)\n",
    "        top_k_indexes = top_indexes[0, -at:]\n",
    "        print(top_k_indexes.shape)\n",
    "\n",
    "        ## Translate to indexes\n",
    "        recommendations = self.target_item_ids[top_k_indexes]\n",
    "        \n",
    "        ## RETURN RECOMMENDATIONS\n",
    "        return(recommendations)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.dataset[user_id]\n",
    "        print(\"User profile: %s\" %(user_profile))\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "        print(\"Scores: %s\" %(scores))\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(user_id, ranking)\n",
    "        \n",
    "        print(\"Ranking: %s\" %(ranking))\n",
    "        \n",
    "        export = [0,0,0,0,0]\n",
    "        for i in range(5):\n",
    "            t_id = track_to_id[ranking[i]]\n",
    "            export[i] = t_id\n",
    "            \n",
    "        return export\n",
    "    def _filter_seen(self, user_id, ranking):\n",
    "        user_profile = self.dataset[user_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]\n",
    "\n",
    "print(\"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 4]\n",
      "[4 6 7]\n"
     ]
    }
   ],
   "source": [
    "##TESTING\n",
    "\n",
    "\n",
    "user_weights = [1,2,4,3,7,6,1]\n",
    "target_item_ids = np.array([1,2,4,3,7,6,1])\n",
    "at = 3\n",
    "## ARGSORT BASED ON AXIS = 0, GET [1,0:at]\n",
    "top_indexes = np.argsort(user_weights)[-at:]\n",
    "print(top_indexes)\n",
    "## Translate to indexes\n",
    "recommendations = target_item_ids[top_indexes]\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_target_item_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-74de1a9d553f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_item_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_target_item_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracks_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_item_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_item_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_item_filter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_target_item_filter' is not defined"
     ]
    }
   ],
   "source": [
    "target_item_filter = get_target_item_filter(tracks_final.shape[0])\n",
    "print(target_item_filter.shape)\n",
    "print(track_ids.shape)\n",
    "target_item_ids = track_ids[target_item_filter]\n",
    "\n",
    "#print(target_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ISimilarity(object):\n",
    "    \"\"\"Abstract interface for the similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, shrinkage=10):\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def compute(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cosine(ISimilarity):\n",
    "    def compute(self, X):\n",
    "        # convert to csc matrix for faster column-wise operations\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "        print(\"Converted to csc.\")\n",
    "        # 1) normalize the columns in X\n",
    "        # compute the column-wise norm\n",
    "        # NOTE: this is slightly inefficient. We must copy X to compute the column norms.\n",
    "        # A faster solution is to  normalize the matrix inplace with a Cython function.\n",
    "        Xsq = X.copy()\n",
    "        Xsq.data **= 2\n",
    "        norm = np.sqrt(Xsq.sum(axis=0))\n",
    "        norm = np.asarray(norm).ravel()\n",
    "        norm += 1e-6\n",
    "        # compute the number of non-zeros in each column\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(X.indptr)\n",
    "        # then normalize the values in each column\n",
    "        X.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "\n",
    "        # 2) compute the cosine similarity using the dot-product\n",
    "        dist = X * X.T\n",
    "        print(\"Computed\")\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        dist = dist - sps.dia_matrix((dist.diagonal()[scipy.newaxis, :], [0]), shape=dist.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            dist = self.apply_shrinkage(X, dist)\n",
    "            print(\"Applied shrinkage\")    \n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n",
    "    \n",
    "    def remove_noise(self, X, noise):\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "        i = 0\n",
    "        for row in X:\n",
    "            r = row\n",
    "            row[row > noise] = 1\n",
    "            row[row <= noise] = 0\n",
    "\n",
    "            X[i,:] = r[row]\n",
    "            i += 1\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87084593  0.25126765  0.51575033]]\n",
      "0.51575033144\n",
      "[[ 0.63577798  0.08539221  0.36355882]]\n",
      "0.363558821693\n",
      "[[ 0.09416532  0.22020453  0.12421781]]\n",
      "0.12421781415\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def kkeep_k_largest(X, k):\n",
    "    \n",
    "    M = X.todense()\n",
    "    for row in M: \n",
    "        top_k_idx = np.argsort(row)\n",
    "        print(row)\n",
    "        print(row[0,top_k_idx[0,-k]])\n",
    "        \n",
    "    \n",
    "    \n",
    "a = sps.csr_matrix(np.random.rand(3,3))\n",
    "print(kkeep_k_largest(a,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creating a Item-Item Similarity Matrix based on Collaborative Filtering. \n",
    "class CF(object): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def Item_Similarity(self, URM, k = 100, shrinkage = 10):\n",
    "        #Takes a URM (U x I), returns ISM (I x I)\n",
    "        \n",
    "        self.shrinkage = shrinkage\n",
    "        \n",
    "        # We explore the matrix column-wise\n",
    "        URM = check_matrix(URM, 'csc')\n",
    "\n",
    "        n_items = URM.shape[1]\n",
    "\n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        processedItems = 0\n",
    "\n",
    "        # Compute all similarities for each item using vectorization\n",
    "        for itemIndex in range(n_items):\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % 100==0:\n",
    "\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "            # All ratings for a given item\n",
    "            item_ratings = URM[:,itemIndex]\n",
    "            item_ratings = item_ratings.toarray().squeeze()\n",
    "            #print(item_ratings)\n",
    "\n",
    "            # Compute item similarities\n",
    "            this_item_weights = URM.T.dot(item_ratings)\n",
    "            \n",
    "            #print(this_item_weights)\n",
    "\n",
    "            # Sort indices and select TopK\n",
    "            top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "            # Incrementally build sparse matrix\n",
    "            #print(top_k_idx)\n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(URM.shape[1])[top_k_idx])\n",
    "            cols.extend(np.ones(k) * itemIndex)\n",
    "          \n",
    "            \n",
    "        W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                                shape=(n_items, n_items),\n",
    "                                dtype=np.float32)\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        W_sparse = W_sparse - sps.dia_matrix((W_sparse.diagonal()[scipy.newaxis, :], [0]), shape=W_sparse.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        W_sparse.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if shrinkage > 0:\n",
    "            W_sparse = self.apply_shrinkage(URM, W_sparse)\n",
    "            print(\"Applied shrinkage\") \n",
    "\n",
    "        return W_sparse\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 1]\n",
      " [0 0 1 0 1 1]\n",
      " [1 0 1 0 1 0]]\n",
      "Removed diagonal\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-11a0d91352a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mISM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItem_Similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURM_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrinkage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mISM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mISM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-d33ee41652a7>\u001b[0m in \u001b[0;36mItem_Similarity\u001b[0;34m(self, URM, k, shrinkage)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Removed diagonal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mW_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_nnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Normalized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm' is not defined"
     ]
    }
   ],
   "source": [
    "URM = np.matrix([[1, 0, 1, 0, 0, 1], [0, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0]])\n",
    "print(URM)\n",
    "cf = CF()\n",
    "URM_sparse = sps.csc_matrix(URM, dtype = int)\n",
    "\n",
    "\n",
    "ISM = cf.Item_Similarity(URM_sparse, k = 4, shrinkage = 0)\n",
    "print(ISM.nnz)\n",
    "print(ISM.todense())\n",
    "print(ISM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "cdef class Cosine_Similarity:\n",
    "\n",
    "    cdef int TopK\n",
    "    cdef long n_items\n",
    "\n",
    "    # Arrays containing the sparse data\n",
    "    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n",
    "    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n",
    "    cdef double[:] user_to_item_data, item_to_user_data\n",
    "\n",
    "    # In case you select no TopK\n",
    "    cdef double[:,:] W_dense\n",
    "\n",
    "    \n",
    "    def __init__(self, URM, TopK = 100):\n",
    "        \"\"\"\n",
    "        Dataset must be a matrix with items as columns\n",
    "        :param dataset:\n",
    "        :param TopK:\n",
    "        \"\"\"\n",
    "\n",
    "        super(Cosine_Similarity, self).__init__()\n",
    "\n",
    "        self.n_items = URM.shape[1]\n",
    "\n",
    "        self.TopK = min(TopK, self.n_items)\n",
    "\n",
    "        URM = URM.tocsr()\n",
    "        self.user_to_item_row_ptr = URM.indptr\n",
    "        self.user_to_item_cols = URM.indices\n",
    "        self.user_to_item_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        URM = URM.tocsc()\n",
    "        self.item_to_user_rows = URM.indices\n",
    "        self.item_to_user_col_ptr = URM.indptr\n",
    "        self.item_to_user_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_items,self.n_items))\n",
    "\n",
    "\n",
    "\n",
    "    cdef int[:] getUsersThatRatedItem(self, long item_id):\n",
    "        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n",
    "\n",
    "    cdef int[:] getItemsRatedByUser(self, long user_id):\n",
    "        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n",
    "\n",
    "    \n",
    "    \n",
    "    cdef double[:] computeItemSimilarities(self, long item_id_input):\n",
    "        \"\"\"\n",
    "        For every item the cosine similarity against other items depends on whether they have users in common. \n",
    "        The more common users the higher the similarity.\n",
    "        \n",
    "        The basic implementation is:\n",
    "        - Select the first item\n",
    "        - Loop through all other items\n",
    "        -- Given the two items, get the users they have in common\n",
    "        -- Update the similarity considering all common users\n",
    "        \n",
    "        That is VERY slow due to the common user part, in which a long data structure is looped multiple times.\n",
    "        \n",
    "        A better way is to use the data structure in a different way skipping the search part, getting directly\n",
    "        the information we need.\n",
    "        \n",
    "        The implementation here used is:\n",
    "        - Select the first item\n",
    "        - Initialize a zero valued array for the similarities\n",
    "        - Get the users who rated the first item\n",
    "        - Loop through the users\n",
    "        -- Given a user, get the items he rated (second item)\n",
    "        -- Update the similarity of the items he rated\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create template used to initialize an array with zeros\n",
    "        # Much faster than np.zeros(self.n_items)\n",
    "        cdef array[double] template_zero = array('d')\n",
    "        cdef array[double] result = clone(template_zero, self.n_items, zero=True)\n",
    "\n",
    "\n",
    "        cdef long user_index, user_id, item_index, item_id_second\n",
    "\n",
    "        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n",
    "        cdef int[:] items_rated_by_user\n",
    "\n",
    "        cdef double rating_item_input, rating_item_second\n",
    "\n",
    "        # Get users that rated the items\n",
    "        for user_index in range(len(users_that_rated_item)):\n",
    "\n",
    "            user_id = users_that_rated_item[user_index]\n",
    "            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n",
    "\n",
    "            # Get all items rated by that user\n",
    "            items_rated_by_user = self.getItemsRatedByUser(user_id)\n",
    "\n",
    "            for item_index in range(len(items_rated_by_user)):\n",
    "\n",
    "                item_id_second = items_rated_by_user[item_index]\n",
    "\n",
    "                # Do not compute the similarity on the diagonal\n",
    "                if item_id_second != item_id_input:\n",
    "                    # Increment similairty\n",
    "                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n",
    "\n",
    "                    result[item_id_second] += rating_item_input*rating_item_second\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def compute_similarity(self):\n",
    "\n",
    "        cdef int itemIndex, innerItemIndex\n",
    "        cdef long long topKItemIndex\n",
    "\n",
    "        cdef long long[:] top_k_idx\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\n",
    "\n",
    "        #cdef long[:] top_k_idx\n",
    "        cdef double[:] this_item_weights\n",
    "\n",
    "        cdef long processedItems = 0\n",
    "\n",
    "        # Data structure to incrementally build sparse matrix\n",
    "        # Preinitialize max possible length\n",
    "        cdef double[:] values = np.zeros((self.n_items*self.TopK))\n",
    "        cdef int[:] rows = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef int[:] cols = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef long sparse_data_pointer = 0\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute all similarities for each item\n",
    "        for itemIndex in range(self.n_items):\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % 10000==0 or processedItems==self.n_items:\n",
    "\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print(\"Similarity item {} ( {:2.0f} % ), {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, processedItems*1.0/self.n_items*100, itemPerSec, (self.n_items-processedItems) / itemPerSec / 60))\n",
    "\n",
    "            this_item_weights = self.computeItemSimilarities(itemIndex)\n",
    "\n",
    "            if self.TopK == 0:\n",
    "\n",
    "                for innerItemIndex in range(self.n_items):\n",
    "                    self.W_dense[innerItemIndex,itemIndex] = this_item_weights[innerItemIndex]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                # top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "\n",
    "                this_item_weights_np = - np.array(this_item_weights)\n",
    "                \n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(this_item_weights_np, self.TopK-1)[0:self.TopK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "\n",
    "\n",
    "                # Incrementally build sparse matrix\n",
    "                for innerItemIndex in range(len(top_k_idx)):\n",
    "\n",
    "                    topKItemIndex = top_k_idx[innerItemIndex]\n",
    "\n",
    "                    values[sparse_data_pointer] = this_item_weights[topKItemIndex]\n",
    "                    rows[sparse_data_pointer] = topKItemIndex\n",
    "                    cols[sparse_data_pointer] = itemIndex\n",
    "\n",
    "                    sparse_data_pointer += 1\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "\n",
    "            return np.array(self.W_dense)\n",
    "\n",
    "        else:\n",
    "\n",
    "            values = np.array(values[0:sparse_data_pointer])\n",
    "            rows = np.array(rows[0:sparse_data_pointer])\n",
    "            cols = np.array(cols[0:sparse_data_pointer])\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                    shape=(self.n_items, self.n_items),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 1]\n",
      " [0 0 1 0 1 1]\n",
      " [1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "URM = np.matrix([[1, 0, 1, 0, 0, 1], [0, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0]])\n",
    "print(URM)\n",
    "cf = CF()\n",
    "URM_sparse = sps.csc_matrix(URM, dtype = int)\n",
    "\n",
    "\n",
    "#ISM = cf.Item_Similarity(URM_sparse, k = 4, shrinkage = 0)\n",
    "#print(ISM.nnz)\n",
    "#print(ISM.todense())\n",
    "#print(ISM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 756670. False: 188909. Tot: 945579\n",
      "Built URM_test\n",
      "Total datapoints: 945579. Expected: 945579\n",
      "(57560, 74542)\n",
      "(57560, 74539)\n",
      "Similarity item 10000 ( 13 % ), 2496.28 item/sec, required time 0.43 min\n",
      "Similarity item 20000 ( 27 % ), 2505.06 item/sec, required time 0.36 min\n",
      "Similarity item 30000 ( 40 % ), 2522.97 item/sec, required time 0.29 min\n",
      "Similarity item 40000 ( 54 % ), 2515.79 item/sec, required time 0.23 min\n",
      "Similarity item 50000 ( 67 % ), 2501.56 item/sec, required time 0.16 min\n",
      "Similarity item 60000 ( 80 % ), 2502.78 item/sec, required time 0.10 min\n",
      "Similarity item 70000 ( 94 % ), 2505.40 item/sec, required time 0.03 min\n",
      "Similarity item 74542 ( 100 % ), 2504.62 item/sec, required time 0.00 min\n",
      "Similarity computed in 30.82 seconds\n",
      "(74542, 74542)\n",
      "3727100\n"
     ]
    }
   ],
   "source": [
    "URM_train, URM_test = build_URM(0.8)\n",
    "\n",
    "cosine_cython = Cosine_Similarity(URM_train, TopK=50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ISM = cosine_cython.compute_similarity()\n",
    "\n",
    "print(\"Similarity computed in {:.2f} seconds\".format(time.time()-start_time))\n",
    "print(ISM.shape)\n",
    "print(ISM.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with train_rate 1\n",
      "True: 945579. False: 0. Tot: 945579\n",
      "Total datapoints: 945579. Expected: 945579\n",
      "(57560, 74542)\n",
      "(10, 10)\n",
      "Created filter preserving 32195 out of 74542 \n",
      "Using <__main__.Cosine object at 0x1a18539e80> filtering with TopK = 100 to compute distance.\n",
      "Converted to csc.\n",
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Applied shrinkage\n",
      "Computed Item-Item similarity matrix. 156.81023001670837 \n",
      "945579\n",
      "1076561936\n",
      "Filtered targeted tracks in ISM. 28.735347986221313 \n",
      "945579\n",
      "480359576\n",
      "Checked URM csr 0.07162117958068848 \n",
      "(57560, 74542)\n",
      "(74542, 32195)\n"
     ]
    }
   ],
   "source": [
    "#This is the main script! \n",
    "\n",
    "\n",
    "#1. Fitting the model. \n",
    "\n",
    "#If export is true, the recommendations will be written to file. \n",
    "#If false, evaluation method can be used. \n",
    "export = True\n",
    "\n",
    "if export:\n",
    "    train_rate = 1\n",
    "else:\n",
    "    train_rate = 0.8\n",
    "print(\"Running with train_rate %s\" %(train_rate))\n",
    "\n",
    "URM_train, URM_test = build_URM(train_rate)\n",
    "\n",
    "import time\n",
    "starttime = time.time()\n",
    "rec = Recommender(URM=URM_train, \n",
    "                  target_items = target_tracks, \n",
    "                  item_ids = track_ids, \n",
    "                  shrinkage=40.0, \n",
    "                  distance = 'collaborative',\n",
    "                  topK = 100)\n",
    "#ICM_idf = ICM_add_IDF(ICM_all)\n",
    "ISM = rec.fit_new(ICM_all) ##Saving outside for quicker restarts. \n",
    "#rec.fit_bad(ICM_all, k = 2000)\n",
    "print(\"Fitted in %s seconds\" %(time.time()-starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10000 playlists, 0.0016570091247558594 sec.\n",
      "1000 out of 10000 playlists, 2.647590160369873 sec.\n",
      "2000 out of 10000 playlists, 5.019283294677734 sec.\n",
      "3000 out of 10000 playlists, 7.276247024536133 sec.\n",
      "4000 out of 10000 playlists, 9.58328127861023 sec.\n",
      "5000 out of 10000 playlists, 11.81442403793335 sec.\n",
      "6000 out of 10000 playlists, 13.980063199996948 sec.\n",
      "7000 out of 10000 playlists, 16.11404514312744 sec.\n",
      "8000 out of 10000 playlists, 18.29221224784851 sec.\n",
      "9000 out of 10000 playlists, 20.396782159805298 sec.\n",
      "Saved to file: \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#2. Creating recommendations. \n",
    "zeros = np.zeros((target_playlists.size, 6), dtype = int)\n",
    "recommendations = pd.DataFrame(zeros)\n",
    "recommendations.columns = ['playlist_id', 1, 2, 3, 4, 5]\n",
    "\n",
    "counter = 0\n",
    "starttime = time.time()\n",
    "for playlist_id in target_playlists['playlist_id']:\n",
    "\n",
    "    if counter % 1000 == 0: \n",
    "        print (\"%s out of 10000 playlists, %s sec.\" %(counter, time.time()-starttime))\n",
    "\n",
    "    playlist_id_translated = playlist_to_index[int(playlist_id)]\n",
    "    recommendations.iloc[counter, 1:6] = rec.recommend_new(playlist_id, 5)\n",
    "    recommendations.iloc[counter, 0] = playlist_id\n",
    "    counter += 1\n",
    "\n",
    "if export:\n",
    "    filename = \"recommendations_6/11_\"\n",
    "    np.savetxt(\"output/recommendations_nov_5.csv\",recommendations, fmt = '%s,%s %s %s %s %s', header = \"playlist_id,track_ids\", newline = \"\\n\")\n",
    "    print(\"Saved to file: \")\n",
    "\n",
    "#print(recommendations)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57557, 100000)\n",
      "(57560, 100000)\n"
     ]
    }
   ],
   "source": [
    "# 3. Want to evaluate? \n",
    "print(URM_test.shape)\n",
    "print(URM_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57557, 100000)\n",
      "(57560, 100000)\n",
      "User 0 of 10000, 0 sec.\n",
      "User 500 of 10000, 0 sec.\n",
      "User 1000 of 10000, 1 sec.\n",
      "User 1500 of 10000, 1 sec.\n",
      "User 2000 of 10000, 1 sec.\n",
      "User 2500 of 10000, 1 sec.\n",
      "User 3000 of 10000, 1 sec.\n",
      "User 3500 of 10000, 2 sec.\n",
      "User 4000 of 10000, 2 sec.\n",
      "User 4500 of 10000, 2 sec.\n",
      "User 5000 of 10000, 2 sec.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index (57559) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-96c640e0a161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURM_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mevaluate_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURM_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-b66d963f2241>\u001b[0m in \u001b[0;36mevaluate_algorithm\u001b[0;34m(URM_test, recommendations, at)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User %d of %d, %d sec.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaylists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mrelevant_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mURM_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplaylist_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplaylist_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# [i, 1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_row_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0;31m# [i, [1, 2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0missequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m_get_row_slice\u001b[0;34m(self, i, cslice)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcslice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (57559) out of range"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm(URM_test, recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 50000 were not in the target.\n"
     ]
    }
   ],
   "source": [
    "# Does the recommender rec just targeted tracks? \n",
    "def test_all_rec_in_target(recommendations):\n",
    "    tt = target_tracks.values\n",
    "    recommendations = recommendations.as_matrix()\n",
    "    notcount = 0\n",
    "    count = 0\n",
    "    for row in recommendations: \n",
    "        for item in row[1:6]: \n",
    "            count += 1\n",
    "            if item not in tt: \n",
    "                notcount += 1\n",
    "                #print(\"Rec not in target! %s\" %item)\n",
    "    print(\"%s out of %s were not in the target.\" %(notcount, count))\n",
    "    \n",
    "test_all_rec_in_target(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TESTING THE REC FUNCTION - SHOULD WORK\n",
    "#Fitted in 172.8 seconds\n",
    "\n",
    "rec_dev = Recommender(URM=URM_train, target_items = target_tracks, item_ids = track_ids, shrinkage=0.0)\n",
    "rec_dev.UIM = rec.UIM\n",
    "rec_dev.target_item_ids = rec.target_item_ids\n",
    "\n",
    "zeros = np.zeros((1, 6), dtype = int)\n",
    "recommendations = pd.DataFrame(zeros)\n",
    "recommendations.columns = ['playlist_id', 1, 2, 3, 4, 5]\n",
    "recommendations.iloc[counter, 1:6] = rec_dev.recommend_dev(playlist_to_id[30680], 5)\n",
    "recommendations.iloc[counter, 0] = playlist_to_id[30680]\n",
    "\n",
    "print(recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "rec = Recommender(URM=URM_train, shrinkage=0.0)\n",
    "rec.fit(ICM_all)\n",
    "print(\"Done in %s seconds\" %(time.time()-starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ICM_add_IDF(ICM): \n",
    "    num_tot_items = ICM_all.shape[0]\n",
    "\n",
    "    # let's count how many items have a certain feature\n",
    "    items_per_feature = (ICM_all > 0).sum(axis=0)\n",
    "\n",
    "    IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "\n",
    "    print(ICM_all.shape)\n",
    "    print(IDF.shape)\n",
    "    ICM_idf = sps.csr_matrix(ICM_all, dtype=np.float64)\n",
    "    # compute the number of non-zeros in each col\n",
    "    # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "    col_nnz = np.diff(check_matrix(ICM_idf, 'csc').indptr)\n",
    "    print(col_nnz.shape)\n",
    "    print(ICM_idf.shape)\n",
    "    print(IDF.shape)\n",
    "    # then normalize the values in each col\n",
    "    ICM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "    return ICM_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_idf = BasicItemKNNRecommender(URM=URM_train, shrinkage=0.0, k=50)\n",
    "rec_idf.fit(ICM_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_algorithm(URM_test, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_file():\n",
    "    #Saves the recommendations dataframe to the .csv-file. \n",
    "    np.savetxt(\"output/recommendations_more_content.csv\",recommendations, fmt = '%s,%s %s %s %s %s', header = \"playlist_id,track_ids\", newline = \"\\n\")\n",
    "    \n",
    "    \n",
    "def test():\n",
    "    #Do something\n",
    "    print(\"Result: \")\n",
    "    pass\n",
    "\n",
    "\n",
    "save_to_file()\n",
    "print(recommendations.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
